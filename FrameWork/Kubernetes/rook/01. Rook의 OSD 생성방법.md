---
tags:
  - FrameWork
  - Kubernetes
  - Rook
---
### 환경 구성
* ubuntu 22.04
* cri-o
* calico
* kubernentes v1.28

### OSD(Object Storage Daemon)
`OSD`은 데이터를 저장하고 replication이나, recovery, rebalancing을 수행합니다.
또한 각각의 데몬들의 hearbeat를 Ceph Monitors and Managers에 제공합니다.
최소 3개 이상의 osd로 구성하는게 좋습니다.

공식문서에 따르면 `osd`를 생성하는 방법은 다양합니다. [공식문서](https://rook.io/docs/rook/latest-release/Getting-Started/quickstart/?h=lvm#prerequisites)
노별로 특정해서 설정할 수 있고, 디바이스를 장착해 설정하거나 LVM으로 설정하고 등등 말이죠.

하지만, 이 모든 과정에서의 전제는 해당 파일시스템이 포멧되어 있는 상태이면 안됩니다.

포멧되어있지 않은 상태란, 마운트가 불가능한 상태이며 `ext4` or `nfts` or `exFAT` 등으로 설정되지 않은 깡통 상태를 말합니다.

### Raw devices (no partitions or formatted filesystem)
원시 장치를 추가하는 것으로 `OSD`를 추가할 수 있습니다.

```bash
kubectl create -f cluster.yaml
```

이 경우 위의 명령어를 통해 자동으로 추가되게 됩니다.

단, 위에 `OSD`에서 설명한 것과 같이 파일시스템이 포멧되어 있는 상태이면 안됩니다.

### Raw partitions (no ==formatted== filesystem)
원시 장치를 추가하는 것으로 `OSD`를 추가할 수 있습니다.

```bash
kubectl create -f cluster.yaml
```

이 경우 위의 명령어를 통해 자동으로 추가되게 됩니다.

단, 위에 `OSD`에서 설명한 것과 같이 파일시스템이 포멧되어 있는 상태이면 안됩니다.

### LVM Logical Volumes (no ==formatted== filesystem)
`LV`의 경우는 위의 두 경우와는 다릅니다.

작성된 명령어를 통해 클러스터가 생성될 때 JOB이 동작하여 디바이스를 탐색해 추가하는 방식으로 동작하기 때문에 커스텀해서 사용하지 않는 이상 전부 등록됩니다.

`LV`의 경우 자동으로 탐색하지 않습니다. 이유는 공식문서에서도 찾지를 못했습니다...

자 이제 `LV`로 `OSD`를 구축해 보도록 하겠습니다.
우선 우리는 `cluster.yaml`파일에서 설정을 해주어야 합니다.

```bash
vi rook/deploy/example/cluster.yaml
```

가장 먼저 처리해주어야 할 부분은 `mon`과 `mgr`의 `allowMultiplePerNode`의 값을 `true`로 변경하는 작업입니다. 데이터 손실이 발생하는 것을 허용하는 것인데 이걸 `true`로 하지 않으면 동작하지 않습니다.

파일을 오픈하면 `spec.sotrage`부분을 찾아야합니다.
해당 부분을 찾으면 다음과 같은 부분이 존재합니다.

```yaml
storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: true
    #deviceFilter:
    config:
      # crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
      # metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      # osdsPerDevice: "1" # this value can be overridden at the node or device level
      # encryptedDevice: "true" # the default value for this option is "false"
    # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
    # nodes below will be used as storage resources.  Each node's 'name' field should match their 'kubernetes.io/hostname' label.
    # nodes:
    #   - name: "172.17.4.201"
    #     devices: # specific devices to use for storage can be specified for each node
    #       - name: "sdb"
    #       - name: "nvme01" # multiple osds can be created on high performance devices
    #         config:
    #           osdsPerDevice: "5"
    #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
    #     config: # configuration can be specified at the node level which overrides the cluster level config
    #   - name: "172.17.4.301"
    #     deviceFilter: "^sd."
    # when onlyApplyOSDPlacement is false, will merge both placement.All() and placement.osd
    onlyApplyOSDPlacement: false
    # Time for which an OSD pod will sleep before restarting, if it stopped due to flapping
    # flappingRestartIntervalHours: 24
    # The ratio at which Ceph should block IO if the OSDs are too full. The default is 0.95.
    # fullRatio: 0.95
    # The ratio at which Ceph should stop backfilling data if the OSDs are too full. The default is 0.90.
    # backfillFullRatio: 0.90
    # The ratio at which Ceph should raise a health warning if the OSDs are almost full. The default is 0.85.
    # nearFullRatio: 0.85
```

20번째 줄을 확인해보면 path를 통해 디바이스를 잡아주는 방법이 보입니다.
우리가 활용할 방법은 저 방법입니다. 그러면 다음 명령어롤 통해 path를 확인해 봅시다.

```bash
sudo fdisk -l

Disk /dev/mapper/ubuntu--vg-volume01: 15 GiB, 16106127360 bytes, 31457280 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/mapper/ubuntu--vg-volume02: 15 GiB, 16106127360 bytes, 31457280 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes


Disk /dev/mapper/ubuntu--vg-lv--0: 19 GiB, 20401094656 bytes, 39845888 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
```

`/dev/mapper/ubuntu--vg-volume01` 이 부분이 보이시나요???
우리는 이부분이 path가 됩니다.
그러면 `cluster.yaml`파일에 작성합니다.

참고해야할 사항은 `useAllDevices`속성을 `false`로 변경해야 한다는 겁니다.
수동으로 설정할 때에는 `false`로 지정해야 모든 디바이스를 검색하여 설치하지 않습니다.

```yaml
 storage: # cluster level storage configuration and selection
    useAllNodes: true
    useAllDevices: false
    devices: # specific devices to use for storage can be specified for each node
      - name: "/dev/mapper/ubuntu--vg-volume01"
      - name: "/dev/mapper/ubuntu--vg-volume02"
      - name: "/dev/mapper/ubuntu--vg-lv--0"
    #deviceFilter:
```

그러면 이제 `cluster.yaml`파일로 클러스터를 생성해 보겠습니다.

```bash
kubectl create -f cluster.yaml
```

### Persistent Volumes available from a storage class in 'block' mode
TODO:차후 업데이트 예정...

### 마무리
아래 명령어를 입력 후에 우리가 생성한 lv에 `ceph`포멧형식을 달고 있다면 성공!!

```bash
lsblk -f

# result
sda
├─sda1
│
├─sda2
│    ext4   1.0         b4bec748-d5ef-4930-b4a1-895218cbe8bc      1.7G     7% /boot
└─sda3
     LVM2_m LVM2        xfOLjT-HhPZ-CPq0-6fv1-Y8oa-8zhr-oRwOBk
  ├─ubuntu--vg-ubuntu--lv
  │  ext4   1.0         b4827b45-1fcd-4b2f-8a57-b80fc5b3fb11     31.9G    28% /var/lib/containers/storage/overlay
  │                                                                           /
  ├─ubuntu--vg-volume01
  │  ceph_b
  ├─ubuntu--vg-volume02
  │  ceph_b
  └─ubuntu--vg-lv--0
     ceph_b
```

아래 명령어를 입력 후 osd가 3개가 생성되었다면 성공!!

```bash
ceph status

# result
  services:
    mon: 3 daemons, quorum a,b,c (age 88m)
    mgr: b(active, since 88m), standbys: a
    mds: 1/1 daemons up, 1 standby
    osd: 3 osds: 3 up (since 88m), 3 in (since 16h)
```

---
# Tip!!
### kubectl edit
이미 우리가 클러스터를 생성했고, 그 안에 직접 수정하고 싶을 경우 사용합니다.
물론, `kubectl apply`로 파일을 재적용 시킬수도 있습니다.

```bash
kubectl edit cephcluster -n rook-ceph rook-ceph
```