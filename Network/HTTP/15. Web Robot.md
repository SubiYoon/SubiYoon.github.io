---
tags:
  - HTTP완벽가이드
---

# Web Robot?

- 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램
- 방식에 따라 “크롤러” “웜” “스파이더” “봇” 등 다양하다.
    
    **ex) 검색엔진, 주식 그래프 로봇 등**
    

---

# 크롤러와 크롤링

- 웹 크롤러의 작동 방식 (crawl : 기어다니기)
    1. 페이지를 방문한다.
    2. 방문한 페이지의 하위 페이지를 방문한다.
    3. 하위 페이지의 하위페이지를 방문한다.
    4. 위의 행동을 재귀적으로 반복한다.
    
    **ex) 검색엔진**
    
    → 크롤링을 하면서 만나는 모든 문서를 끌어와 나중에 검색 가능한 데이터베이스로 만들어진다.
    

### 루트 집합

- 크롤링을 시작하는 출발지점
- 몇 몇 페이지들은 성격상 찾아가지 못 할 수 있다. 때문에 신중히 골라야 한다.
    - 인기가 많거나 새로 생성된 페이지들의 목록, 자주 링크되지 않는 잘 알려지져 있지 않은 페이지 등
      <img width="587" alt="스크린샷 2023-08-16 오후 9 51 52" src="https://github.com/SubiYoon/SubiYoon.github.io/assets/117332903/8b9dce5a-6e59-4152-ae8f-8bfff384a84f">

### 링크 추출과 상대 링크 정상화

- 크롤러는 검색한 페이지 안의 URL링크를 파싱하여 크롤링할 리스트에 추가해야 한다.
- 새로운 링크를 받게되면 이 목록은 급속히 확장해 간다.
- 순환 피하기(무한 루프…) 위해 로봇들은 반드시 어디를 방문했는지 알아야 한다.

### 방문한 곳을 관리하기 위한 기법들

- **트리와 해시 테이블**
    - 검색트리와 해시 테이블을 이용하여 URL을 훨씬 빨리 찾게 해주는 소프트웨어 자료구조
- **느슨한 존재 비트맵**
    - 공간 활용을 위해 사용
    - 존재 비트 배열(presence bit array)과 같은 느슨한 자료 구조를 사용
    - 각 URL은 해시함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응 되는 존재 비트를 갖는다.
    - 만약 이 존재비트가 이미 존재한다면, 해당 URL은 크롤링 되었다고 간주하게 된다.
- **체크포인트**
    - 갑작스러운 중단을 대비하여 디스크에 방문한 URL 목록이 저장되었는지 확인한다.
- **파티셔닝**
    - 웹이 성장하면서 하나의 컴퓨터에서 하나의 로봇이 클로링을 완수하는 것은 불가능해 졌다.
        - 한대의 컴퓨터로 하기엔 하드웨어적인 성능에 대한 한계가 있기 때문
    - **“**농장**”**이란 개념과 비슷하게 특정 기준으로 여러 로봇들이 서로 도와가며 크롤링을 한다.
        - 서로 커뮤니케이션을 한다.

### 별칭(alias)과 로봇 순환

- 올바른 자료 구조를 갖추었더라도 해당 URL이 별칭을 가질 수 있는 이상 확실히 구분 하기는 힘들다.
    
    <img width="676" alt="스크린샷 2023-08-16 오후 9 52 46" src="https://github.com/SubiYoon/SubiYoon.github.io/assets/117332903/84fc6098-0875-498e-8982-423e0aa00388">

### URL정규화

- 다음과 같은 세가지로 위의 a ~ c 까지의 문제들을 제거 할 수 있다.
    1. 포트가 명시되어 있지 않았다면, 호스트 명에 **“**:80**”**을 추가한다.
    2. 모든 %xx 이스케이핑된 문자들을 대응되는 문자로 변환한다.
    3. **“**#**”** 태그들을 제거한다.
- 하지만, 각 Web Server에 대한 지식 없이 로봇이 d ~ f 까지의 문제를 해결할 방법은 없다.
- 따라서, 기본적인 문법의 별칭은 제거가 가능하지만 제거할 수 없는 다른 URL  별칭도 만나게 될 것이다.

### 파일 시스템 링크 순환

- **심벌릭 링크**를 사용시 링크 순환이 발생 할 수 있다.
    - **심벌릭 링크**
        
        → 바로 가기 링크와 비슷하다.
        
        → 폴더 or 파일을 자동으로 가르키도록 하는 기능
        
        → Apache의 경우 httpd.conf파일에 명시하여 사용한다.
        
        → Linux에서는 원본파일을 사용하는 것과 같은 모양세를 취하는 것(확장하여 사용할 때 사용)
        
    <img width="678" alt="스크린샷 2023-08-16 오후 9 53 11" src="https://github.com/SubiYoon/SubiYoon.github.io/assets/117332903/fd5a60ba-b48a-4da2-a656-6ebad30d4b94">
    
    - (a)의 경우
        1. /index.html 을 가져와서, subdir/index.html 발견함.
        2. subdir/index.html 가져와서, subdir/logo.gif로 이어지는 링크 발견
        3. subdir/logo.gif을 가져오고 더 이상 링크가 없으므로 완료
    - (b)의 경우
        1. /index.html 을 가져와서, subdir/index.html 발견함.
        2. subdir/index.html을 가져왔지만 같은 index.html로 되돌아감
        3. subdir/subdir/index.html을 가져온다
        4. subdir/subdir/subdir/index.html을 가져오고 무한반복한다.

### 동적 가상 웹 공간

- 악의적인 Web Master들이 순진한 로봇들을 이상한 세계로 보내 버릴 수 있다.
- 다음과 같이 의미없는 것을 악의적으로 만들어 로봇을 이상한 곳으로 보낼 수 있다….

```bash
# 요청 메시지
GET /index-fall.html HTTP/1.1
Host: www.evil-joes-hardware.com
Accept: *
User-agent: ShopBot

# 응답 메시지
HTTP/1.1 200 OK
Content-type: text/html
Content-length: 617

<HTML><BODY>
<A HREF=/index-fail2.html>trick</A>[...]

# 요청 메시지
GET /index-fall2.html HTTP/1.1
Host: www.evil-joes-hardware.com
Accept: *
User-agent: ShopBot

# 응답 메시지
HTTP/1.1 200 OK
Content-type: text/html
Content-length: 617

<HTML><BODY>
<A HREF=/index-fail3.html>trick</A>[...]

# 요청 메시지
GET /index-fall3.html HTTP/1.1
...
```

### **루프의 중복 피하기**

- 모든 순환을 완벽하게 피하는 방법은 없다.
- 다음은 올바르게 동작하기 위해 사용하는 기법이다.
    1. URL정규화
    2. 너비 우선 크롤링
        
        → 순환에 빠지더라도 그 이전에 읽어들인 페이지들은 가져올 수 있다.
        
    3. 스로트링
        
        → 최대 일정 시간동안 가져올 수 있는 갯수를 제한하여 순환에 빠지는 것을 방지
        
    4. URL 크기 제한
        
        → 순환에 빠져 URL이 길어지는 현상을 파악하여 방지할 수 있다.
        
        → 하지만, 실제 유효한 컨텐츠를 제외시킬 수 있는 문제를 야기한다.
        
        → 로그를 남겨 흐름을 파악하면 좀 더 나은 방향으로 나아갈 수 있다.
        
    5. URL/사이트 블랙리스트
        
        → 악의적인 사이트를 블랙리스트에 등록해 해당 사이트 or URL을 발견히 피하며 방지
        
    6. 패턴 발견
        
        → 악의적인 페이지의 URL의 패턴을 분석해 방지
        
    7. 컨텐츠 지문(fingerprint)
        
        → 중복을 감지하는 것보다 직접 적인 방법이다.
        
        → 페이지의 컨텐츠에서 몇 바이트를 얻어내 체크섬(checksum)을 계산
        
        → 해당 체크섬과 같은 것을 발견하면 Pass
        
    8. 사람의 모니터링

---

# Robot’s HTTP

- 로봇들은 일반 HTTP와 다르지 않게 동작한다.
- 하지만, 성능을 위해 간단한 HTTP 요청으로만 해결하는데 이를 위해 HTTP/1.0을 사용한다.

### 요청 헤더 식별하기

- 로봇을 구현할 때 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇가지 헤더를 사이트에 보내는 것이 좋다.
- 종류
    - **User-Agent**
        
        → 요청을 만든 로봇의 이름을 말해준다.
        
    - **From**
        
        → 로봇의 사용자/관리자의 이메일 주소를 제공한다.
        
    - **Accept**
        
        → 어떤 미디어 타입을 보내도 되는지 명시한다.
        
    - **Referer**
        
        → 현재 요청 URL을 포함한 문서의 URL을 제공한다.
        

### 가상 호스팅

- 가상 호스팅이 널리 퍼진 현실에서 Web Robot의 Host헤더를 명시해야 한다.
    - Host 헤더를 포함하지 않으면 로봇이 URL에 대해 잘못된 컨텐츠를 찾게 만든다.
- **가상 호스팅**
    
    → 하나의 Web Server에 여러개의 서버를 올리는 것과 비슷….??
    

### 조건부 요청

- 몇몇 로봇 들은 시간이나 엔터티 태그를 비교함으로써 그들이 받아간 마지막 버전 이후 업데이트 된 것이 있는지 알아보는 조건부 HTTP요청을 구현한다. (Cache의 신선도 검사와 유사??)

### 응답 다루기

- 대부분의 로봇들은 단순히 GET방식의 데이터를 가져오는 요청이 관심사다.
- 몇몇의 로봇은 웹 탐색 or 서버와의 상호작용을 더 잘하기 위해여 HTTP 응답을 다룰 줄 알아야 한다.
    - 상태 코드
    - 엔터티
        
        **ex) http-equiv 태그와 같은 메타 HTML태그의 값을 Header에 갖고 있는 것 처럼 다루어야 한다**
        
        ```html
        <meta http-equiv="Refresh" content="1; URL=index.html"/>
        
        <!--해당 meta data를 다음과 같이 Header로 해석-->
        Refresh: 1; ndex.html
        ```
        

### User-Agent 타기팅

- Web Master들은 로봇이 해당 사이트를 방문할 수도 있다는 것을 인지하고 있어야 한다.
- 풍부한 기능을 가지지 못한 브라우저나 로봇 등 다양한 클라이언트에 잘 대응하는 유연한 페이지를 개발할 수 있어야 한다.

---

# 부적절하게 동작하는 로봇들

- 로봇들이 실수를 저질러 아수라장을 만들 수도 있다.

### 폭주하는 로봇

- 만약 로봇이 순환에 빠졌다면 Web Server에 과부하를 유발하여 다른 누구에게도 서비스를 제공 못하는 경우가 발생할 수 있다. 때문에 모든 로봇의 설계자들은 폭주 방지를 위한 보호장치를 고려하여 설계해야 한다.

### 오래된 URL

- 로봇은 url의 목록을 방문하는데 이 목록이 오래돼어 콘텐츠가 바뀌었다면 로봇은 존재하지 않는 url에 대한 요청을 많이 보낼 수 있다.

### 길고 잘못된 URL

- 순환이나 프로그래밍상의 오류로 인해 로봇은 웹 사이트에게 크고 의미 없는 URL을 요청할 수 있다.

### 호기심이 지나친 로봇

- 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있도록 만들 수도 있다.
- 하이퍼링크가 존재하지도 않는 문서들을 디렉터리의 콘텐츠를 가져오는 방법 등, 긁어올 때 일어날 수 있다.

### 동적 게이트 접근

- 모든 로봇들이 단순히 문서를 요청하는 기능만 있는 것은 아니다.
- 게이트웨이 애플리케이션 콘텐츠에 대한 URL 요청을 할 수도 있다.
    - 특정한 목적에 의해 요청할 것이다. 하지만, 비용이 많이 들 것이다.

---

# 로봇 차단하기

### robots.txt

- 로봇이 서버에 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다.
- 로봇은 서버에 robots.txt 파일을 파싱해서 해당 파일로 접근이 허용되어 있음을 확인하고 진행한다.

### 로봇 표준 차단

- 버전의 이름이 정의되어 있진 않지만, 세가지 버전이 존대한다.
- 오늘날 대부분은 v0.0 or v1.0을 채택했다.
- v2.0은 훨씬 복잡하고 널리 채택되지 못하고 있다.

| 버전 | 이름과 설명 | 날짜 |
| --- | --- | --- |
| 0.0 | 로봇 배제 표준-Disallow 지시자를 지원하는 마틴 코스터(Martin Koster)의 오리지널 robots.txt 매커니즘 | 1994년 06월 |
| 1.0 | 웹 로봇 제어 방법-Allow B지시자의 지원이 추가된 마틴 코스터의 IETF 초안 | 1996년 11월 |
| 2.0 | 로봇 차단을 위한 확장 표준-정규식과 타이밍 정보를 포함한 숀 코너(Sean Conner)의 확장. | 1996년 11월 |

### 웹 사이트와 robots.txt 파일들

- URL을 방문하기 전에 그 웹사이트에 해당 파일이 존재하면, 로봇은 반드시 해당 파일을 읽고 처리해야한다.
- HTTP의 GET메서드를 통해 가져오고 해당 파일을 text/plain 본문으로 반환한다.
- 로봇은 관리자가 로봇의 접근을 추적할 수 있도록 From이나 User-Agent 헤더를 통해 신원 정보를 넘긴다.

### 응답코드

- 많은 웹사이트가 존재하지만, 모든 웹사이트가 robot.txt파일을 갖고 있지않고, 이 사실은 로봇은 모른다.
- **HTTP 2XX Code**
    - 콘텐츠를 파싱하여 차단 규칙을 얻고 사이트에서 무언가를 가져오려 할 때 그 규칙을 따른다.
- **HTTP 404 Code**
    - 활성화된 제약규칙이 없다고 판별하고 robots.txt의 제약 없이 사이트에 접근한다.
- **HTTP 401 or 403 Code**
    - 해당 서버로 부터 완전한 거부를 뜻한다.
- **HTTP 503 Code**
    - 일시적으로 실패 했다면 해당 사이트의 리소스를 검색하는 것을 미루어야 한다.
- **HTTP 3XX Code**
    - 리다이렉션 상태코드를 받았다면, 리소스가 발견될 때 까지 리다이렉트를 따라가야한다.

### robots.txt의 파일 포멧

- 규칙 줄, 주석 줄, 빈 줄의 세 가지 종류가 있다.
- **규칙 줄**
    - HTTP 헤더처럼 생김 → `<필드>: <값>`
    - 패턴 매칭을 위해 사용
    
    ```bash
    # 이 robots.txt는 Slurp과 WebCrawler가 우리 사이트에 공개된 역역을 클로링하는 것을 허락
    # 그 외의 로봇은 안된다.
    
    User-Agent: slurp
    User-Agent: webcrawler
    Disallow: /private
    
    User-Agent: *
    Disallow:
    ```
    
    - `User-Agent`로 시작하며 URL접근을 알려주는 `Allow` 줄과 `Disallow` 줄이 온다.
    - **User-Agent**
        
        → 로봇의 이름을 명시하는 곳이다.
        
        → 해당 부분은 완전히 일치하는 것이 아닌 부분이 일치하면 된다.
        
        → 또한, 대소문자를 구분하지 않으므로 주의해야할 필요가 있다.
        
        **ex)** `User-Agent: bot`은 Bot, Robot, Bottom, Spambot 등에 매치된다.
        
    - **Disallow / Allow**
        
        → 혀용되는 URL과 허용되지 않은 URL에 대한 경로가 명시적으로 작성되어 있다.
        
        → Disallow와 Allow 순서대로 매칭하며 먼저 일치하는 것이 선택된다.
        
        → 두 조건 모두 맞지 않으면 모든 접근이 허용된다.
        
        → 규칙 경로는 대소문자를 구분하는 접두어로 사용해야 한다.
        
        **ex)** `Disallow: /tmp`는 다음의 모든 URL에 대응한다.
        
        ```bash
        http://www.joe.com/tmp
        http://www.joe.com/tmp/
        http://www.joe.com/tmp/pliers.html
        http://www.joe.com/tmpspc/stuff.txt
        ```
        
        <img width="640" alt="스크린샷 2023-08-16 오후 9 53 34" src="https://github.com/SubiYoon/SubiYoon.github.io/assets/117332903/df9286c3-6a58-4799-a889-7eab15df95ac">
        
---

# 로봇 에티켓

- 마틴 코스터(Martijn Koster)는 웹 로봇을 만드는 사람들을 위한 가이드 라인 목록을 작성했다.
    
    → http://www.robotstxt.org/wc/guidelines.html 에서 찾아 볼 수 있다.
    

### 신원 식별

- 로봇의 신원을 밝혀라!
- 기계의 신원을 밝혀라!
- 연락처를 밝혀라!

### 동작

- 긴장하라!
    - 로봇을 동작하게 되면 문의와 항의가 들어오게 될 것이다.
    - 로봇이 충분히 노련해 지기까지 충분한 감시가 필요하다.
- 대비하라!
    - 로봇이 여행을 떠나기 전 Cost가 사용될 것에 대해 항상 사전에 알리고 대비해야 한다.
- 감시와 로그
    - 진행 상황 추적, 함정 식별 등 모든 것이 정상 작동하는지 기본적인 감시와 로깅 기능이 풍족해야 한다.
- 배우고 조정해라
    - 크롤링을 하면서 새로운 것을 배우고 로봇을 조정해서 함정에 빠지는 것을 방지하라!

### 스스로를 제한하라!

- URL을 필터링하라!
    - 찾는 것이 맞는지 정확히 판별하고 확인해야 한다.
- 동적 URL을 필터링 하라!
    - URL이 `cgi` or `?` 를 포함한다면 크롤링하지 않는 편이 나을 수도 있다.
- Accept관련 헤더로 필터링
    - 로봇은 Accept 관련 헤더들을 이용해 서버에게 어떠한 컨텐츠를 이해할 수 있는지 말해주어야 한다.
- robots.txt에 따르라!
    - 규칙에 따를 줄 알아야 한다.
- 스스로를 억제하라!
    - 해당 웹사이트에 몇번 접근했는지 세고, 이 정보를 이용해 특정 사이트에 자주 방문하지 않도록 처리

### 루프와 중복을 견뎌내기, 그리고 그 외의 문제들

- 모든 응답 코드 다루기!
- URL 정규화 하기
- 적극적으로 순호나 피하기
- 함정을 감시하라!
- 블랙리스트를 관리하라!

### 확장성

- 공간 이해하기
    - 풀고 있는 문제가 얼마나 큰 것인지 미리 계산이 가능해야 한다.
        
        → 많은 메모리를 요구하는 상황이 발생 할 수 있기 때문
        
- 대역폭 이해하기
    - TCP커넥션을 더 효율적으로 사용해야 한다.
- 시간 이해하기
    - 로봇이 작업을 끝내는데 얼마나 많은 시간이 필요한지 이해하고 추측한 시간과 맞는지 검사해보라!
- 분할 정복
    - 대규모 크롤링 사용시 하나의 큰 프로그램 보다 작은 여러개의 프로그램이 더욱 효율적일 수 있다.

### 신뢰성

- 철저하게 테스트하라!
- 체크포인트!
    - 어떤 로봇이 실패한 위치에서 다시 시작할 수 있도록 스냅숏 기능을 사용 할 수 있도록 설계하라!
- 실패에 대한 유연성
    - 실패가 발생하더라도 계속 동작 할 수 있도록 설계하라!

### 소통

- 준비하라!
    - 로봇은 많은 사람들을 화나가 할 것이다. 이에 대해 빠르게 응답할 수 있도록 준비해 두어라!
- 이해하라!
    - 로봇 때문에 화가난 많은 사람들이 나를 대하는 태도에 대해 이해하라….!
- 즉각 대응하라!
    - 웹 마스터들이 로봇에 불만을 갖게 되는 것은 로봇에 대해 정확히 알고 있지 않기 때문이다.
    - 만약 즉각적인 대응을 해줄 수 있다면 항의는 90%이상 감소할 것이다!!!!

---

# 검색 엔진

- 웹 로봇이 가장 많이 쓰이는 곳이 검색엔진 영역이다.

### 현대적인 검색엔진 아키텍처

- 오늘날의 검색엔진은 ‘풀 텍스트 색인(full text indexes)’이라고 하는 로컬 데이터베이스를 생성한다.
    - 이 색인은 웹의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작한다.

### 검색엔진의 동작

- 검색엔진의 크롤러가 웹페이지들을 수집하여 풀 텍스트 색인에 추가한다.
- 구글같은 웹 검색 게이트웨이를 통해 풀 텍스트 색인에 대한 질의를 보낸다.

![스크린샷 2023-08-17 오후 10 12 02](https://github.com/SubiYoon/SubiYoon.github.io/assets/117332903/ff1b7fe2-903e-41b9-952e-2335171da8a3)

### 풀 텍스트 색인(full text indexes)

- 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려 줄 수 있는 데이터베이스
- 각 단어들 포함한 문서들을 열거한다.

![스크린샷 2023-08-17 오후 10 12 22](https://github.com/SubiYoon/SubiYoon.github.io/assets/117332903/8b7e226f-572f-4650-b2d6-de747b44f641)

### 질의보내기

- 사용자가 HTML 폼을 사용자가 채워 넣는다.
- 채워넣은 HTML을 브라우저가 HTTP 요청(GET or POST) 이용해서 게이트웨이에게 보낸다.
- 게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 변환한다.

### 검색 결과를 정렬하고 보여주기

- 검색 결과를 받은 게이트웨이 어플리케이션은 해당 결과를 사용자에게 보여주기 위해 즉석에서 화면을 만든다.
- **관련도 랭킹**이라는 정렬방법을 사용한다.
    - **관련도 랭킹**
        
        → 검색 결과 목록에 점수를 메기고 연관도가 높은 순서대로 정렬하는 과정
        
        → 검색엔진이 웹 크롤링을 하면서 얻은 데이터도 통계적으로 활용하여 정렬시켜 준다.
