---
tags:
  - Kubernetes
  - Rook-Ceph
  - Mirroring
  - Setting
---
## 참고 문서
https://rook.io/docs/rook/v1.9/Storage-Configuration/Block-Storage-RBD/rbd-mirroring/#create-a-volumereplication-cr

## 진행 조건
다음 예시는 k8s-cluster1에 k8s-clster2의 ceph PV 데이터를 미러링하기 위한 방법입니다.

## Helm Values.yaml 수정
기본 Operator의 values를 수정하여야 합니다.
- enableOMAPGernerator
	- Omap 생성기는 CSI 프로비저 포드와 함께 배포될 때 PV와 RBD 이미지 사이에 내부 CSI omaps를 생성하는 사이드카 컨테이너입니다. 이는 DR 사용 사례에서 정적 PV가 피어 클러스터 간에 전송되므로 PVC를 스토리지 매핑에 보존하는 데 필요합니다.

```yaml title:"values-rook-ceph.yaml"
cephBlockPoolsVolumeSnapshotClass:
  enabled: true

cephFileSystemVolumeSnapshotClass:
  enabled: true

csi:
  nfs:
    enabled: true
  enableOMAPGenerator: true
  enableCSIHostNetwork: true

crds:
 enabled: true

monitoring:
  enabled: true
```

현재 제가 진행하는 사항은 서로 다른 물리 PC에서 `Ceph`의 데이터를 미러링하는 상황을 가정했습니다.
미러링을 하기 위해서는 `Ceph` 각 클러스터의 `Monitor(rook-ceph-mon-xxx)`끼리의 통신이 필수입니다. 이 Monitor들은 IP로 서로의 정보를 **map**형태로 주고 받게 되는데, 최초 **Peer**를 맺을때 IP가 서로 접속이 가능해야 합니다. 때문에 `ClusterIP`를 사용하지 않고 `HostNetwork`를 통해 해당 Node의 IP를 할당받아 사용 할 수 있게 수정해야합니다.

아래 옵션중에서는 `cephClusterSpec.network.provider: host`가 해당 값입니다.

```yaml title:"values-rook-ceph-cluster.yaml"
configOverride: |-
  [mgr]
  cephadm_warn_on_stray_daemons = false
  cephadm_warn_on_stray_hosts = false

monitoring:
  createPrometheusRules: false
  enabled: true
  metricsDisabled: true

tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule

toolbox:
  enabled: true
  containerSecurityContext:
    runAsNonRoot: false
    runAsUser: 0
    runAsGroup: 0
    capabilities:
      drop: []

cephClusterSpec:
  network:
    provider: host
  dashboard:
    enabled: true
    port: 20001
    ssl: false
```

## 필요한 Resources 배포
### CephBlockPool
Default로 사용하고 있는 pool을 수정해도 되지만, Mirroring을 전용으로 만들고 싶을 때, 생성하면 됩니다. 저는 전용 pool을 생성하고 진행하겠습니다.

```yaml title:"mirroredpool.yaml"
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: mirroredpool
  namespace: rook-ceph
spec:
  # 복제
  replicated:
    # 개수
    size: 1
  # 미러링  
  mirroring:
    # 사용여부
    enabled: true
    # 방식(pool, image)
    mode: image
```

```bash
kubectl get apply -f mirroredpool.yaml
```

이 과정은 k8s-cluster1 서버와 k8s-cluster2 서버 두 곳에서 실행합니다.

## Bootstrap Peer
rbd-mirror 데몬이 피어 클러스터를 검색하려면 Peer를 등록하고 사용자 계정을 만들어야 합니다.
Peer 클러스터를 부투스트랩하려면 Secret이 필요합니다.

```bash
# k8s-cluster2
kubectl get cephblockpool.ceph.rook.io/mirroredpool -n rook-ceph -ojsonpath='{.status.info.rbdMirrorBootstrapPeerSecretName}'
```

```
# 결과
pool-peer-token-mirroredpool
```

여기 `pool-peer-token-mirroredpool`은 우리가 필요한 Bootstrap의 Secret값을 가지고 있습니다.

```bash
# k8s-cluster2
kubectl get secret -n rook-ceph pool-peer-token-mirroredpool -o jsonpath='{.data.token}'|base64 -d
```

```
# 결과
eyJmc2lkIjoiNGQ1YmNiNDAtNDY3YS00OWVkLThjMGEtOWVhOGJkNDY2OTE3IiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFDZ3hmZGdxN013R0JBQWZzcUtCaGpZVjJUZDRxVzJYQm5kemc9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMzY6MzMwMCx2MToxOTIuMTY4LjM5LjM2OjY3ODldIn0=
```

이젠 디코딩된 값으로 `k8s-cluster1`에서 `secret`을 생성합니다.

```bash
# k8s-clster1
kubectl -n rook-ceph create secret generic rbd-primary-site-secret --from-literal=token=eyJmc2lkIjoiNGQ1YmNiNDAtNDY3YS00OWVkLThjMGEtOWVhOGJkNDY2OTE3IiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFDZ3hmZGdxN013R0JBQWZzcUtCaGpZVjJUZDRxVzJYQm5kemc9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMzkuMzY6MzMwMCx2MToxOTIuMTY4LjM5LjM2OjY3ODldIn0= --from-literal=pool=mirroredpool
```

이 과정은 k8s-cluster1 서버와 k8s-cluster2 서버 두 곳에서 실행합니다.

## RBDMirror Deamon Setting
RBD에 관한 복제는  RBD-mirror를 통해 진행됩니다. 따라서 RBDMirror 데몬을 설치해야합니다. 
RBDMirror Daemon에 관한 CRD는 다음과 같습니다.

```yaml title:"rbd-mirror.yaml"
apiVersion: ceph.rook.io/v1
kind: CephRBDMirror
metadata:
  name: my-rbd-mirror
  namespace: rook-ceph
spec:
  # 배포할 Mirror Daemon의 수
  count: 1
```

```bash
# k8s-cluster1
kubectl apply -f rbd-mirror.yaml
```

```bash
# k8s-cluster1
kubectl get pods -n rook-ceph
```

```
rook-ceph-rbd-mirror-a-6985b47c8c-dpv4k 1/1 Running 0 10s
```

이제 데몬의 상태가 괜찮은지 조회해 보겠습니다.
```bash
kubectl get cephblockpools.ceph.rook.io mirroredpool -n rook-ceph -o jsonpath='{.status.mirroringStatus.summary}'
```

```bash
{"daemon_health":"OK","health":"OK","image_health":"OK","states":{"replaying":1}}
```

## RBD풀에 미러링 피어정보 추가
각 풀은 자체적으로 피어를 가질 수 있는데, 현재 활성화된 `mirroredpool`을 패치하여 피어정보를 추가해 보겠습니다.
우리가 직전에 생성한 `rbd-primary-site-secret`을 추가하면 됩니다.

```bash
# k8s-cluster1
kubectl -n rook-ceph patch cephblockpool mirroredpool --type merge -p '{"spec":{"mirroring":{"peers": {"secretNames": ["rbd-primary-site-secret"]}}}}'
```

---
## Ceph의 백업
#### Velero, MinIO, Restic을 이용하여 백업 및 복원을 도모
미러링을 통해 PV의 데이터를 카피해 보관하고 velero와 MinIO를 통해 Resources들을 백업한 후에 복원하는 과정을 밟을 수 있습니다.

현재는 PV의 `imageName`과 `handler`관련 이슈가 있어 자동화를 못하고 있지만, 차후 해당 부분을 극복 할 수 있으면 업데이트를 진행할 예정입니다.

해당 이슈는 `Rook-Ceph`가 PVC가 생성되면 자동으로 PV를 생성하면서 기존에 사용하는 pool의 `imageName`을 사용하는 것이 아닌 새롭게 할당되어 수동으로 맵핑해주지 않으면 해당 데이터를 온전히 새로운 환경에서 꺼내올 수 없는 이슈가 있습니다.