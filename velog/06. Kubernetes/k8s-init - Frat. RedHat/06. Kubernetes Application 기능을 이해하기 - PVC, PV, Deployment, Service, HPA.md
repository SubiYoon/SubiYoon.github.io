---
tags:
  - FrameWork
  - Kubernetes
  - RedHat
  - PersistentVolumeClaim
  - PersistentVolume
  - Deployment
  - HPA
  - Service
---
## PVC, PV - Pod(hostPath)
### PVC(Persistent Volume Claim)
`PVC`란 일종의 `PV`와 `Pod`간의 명세서 역할을 해주는 Interface입니다.
인프라 담당자가 `Pod`는 개발자가 만들고, `PV`는 인프라 담당자가 만들게 되는데 `PV`가 변경되어도 개발자가 다른 설정사항을 수정하지 않아도 되게 해주는 역할을 하는 것이죠.

#### resource
자원을 정의하는 부분입니다.
`requests`에서 사용할 스토리지의 용량을 `storage`속성을 통해 선택합니다. 
이 값은 `PV`의 `capaticy`의 `storage`값과 일치해야 합니다.

#### accessModes
해당 마운트한 폴더에 접근 권한을 설정하는 부분입니다.
이 값은 `PV`의 `accessModes`의 값과 일치해야 합니다.


```yaml title:PVC.yaml
apiVersion: v1  
kind: PersistentVolumeClaim  
metadata:
  namespace: anotherclass-123  
  name: api-tester-1231-files  
  labels:  
    part-of: k8s-anotherclass  
    component: backend-server  
    name: api-tester  
    instance: api-tester-1231  
    version: 1.0.0  
    managed-by: kubectl  
spec:  
  resources:  
    requests:
      # volume의 용량을 정의  
      storage: 2G  
  # 접근 권한 설정(읽기, 쓰기, 읽기 & 쓰기)
  accessModes:  
    - ReadWriteMany  
  selector:  
    matchLabels:  
      part-of: k8s-anotherclass  
      component: backend-server  
      name: api-tester  
      instance: api-tester-1231-files
```

### PV(Persistent Volume)
`PV`란 `Kubernets`에서 지속적인 저장소 관리를하는 오브젝트입니다.

가령 이런 상황을 상상해봅시다.
보통 `Kubernetes`환경이 아닌 일반 환경에서 운영한다고 친다면, 우리는 `local storage`에 데이터를 저장하거나 파일을 저장할 겁니다.

하지만 `Kubernetes`환경은 조금 다르죠??
장애가 발생하면 `Pod`이 내려가고 새로운 `Pod`이 실행됩니다.
문제는 이 때 내려간 `Pod`의 `local storage`에 데이터를 저장하면 `Pod`이 내려갔을 때, 데이터는 삭제도리 겁니다. 그러면 운영환경에서 큰 문제가 있겠죠??

이 문제를 해결하기 위해 `Pod`이 아닌 특정 `Node`에 데이터를 쓰는 방식을 사용하는 겁니다.
그 `Node`에서 사용하는 `path`와  용량은 얼마나 사용할 것인지를 명세하는 오브젝트입니다.

#### local
`spec`에 있는 속성으로써 하위에 있는 `path`속성을 통해 `Node`의 `Volume path`를 지정합니다.

#### nodeAffinity
어느 `Node`를 지정할 것인지 정하는 속성입니다.
`local`속성을 사용하면 `nodeAffinity`는 반드시 작성해야 합니다.

```yaml title:PV.yaml
apiVersion: v1  
kind: PersistentVolume  
metadata:  
  name: api-tester-1231-files  
  labels:  
    part-of: k8s-anotherclass  
    component: backend-server  
    name: api-tester  
    instance: api-tester-1231-files  
    version: 1.0.0  
    managed-by: dashboard  
spec:  
  capacity:  
    storage: 2G  
  volumeMode: Filesystem  
  accessModes:  
    - ReadWriteMany  
  # 이 path를 Volume으로 사용하겠다는 의미
  local:  
    # 미리 폴더를 만들어 두어야함
    path: "/root/k8s-local-volume/1231"  
  # 대상 Node를 선택하는 것
  nodeAffinity:  
    required:  
      nodeSelectorTerms:  
        - matchExpressions:  
            - {key: kubernetes.io/hostname, operator: In, values: [k8s-master]}
```

![이미지](https://github.com/SubiYoon/SubiYoon.github.io/blob/main/Attached%20File/스크린샷%202025-03-10%20오후%2011.03.01.png?raw=true)

### 참고사항
#### Pod
`Pod`내부에 `volumes`속성에 있는 `hostPath`속성과 `nodeSelector`를 사용해서 `PV`에 설정하는거 처럼 설정 할 수 있습니다.

하지만 이 경우는 `Kubernets`에서 지양하는 사항입니다.

`hostPath,` `local` 사용 목적은 `Node`의 정보를 이용해야하는 기능의 App에서 사용하거나 테스트환경에서 임시 저장 용도로 사용하는 것이 목적입니다.

절대 운영환경에서는 해당 속성으로 사용하지 마시기 바랍니다.
조금은 귀찮더라도 `PVC`와 `PV`를 이용해서 설정해 주시기 바랍니다.

---

## Deployment - update
### Deployment
이번에는 `Deployment`를 사용해 Version up을 하거나 RollBack을 할 경우에 대해 알아 보게습니다.

`Pod`의 업데이트는 `Deployment`의 `template`의 하위 속성값들이 변경되면 발생한다.

#### strategy
이 속성은 `update`시에 어떤 방식으로 `Pod`을 갈아 끼울건지에 대한 설정을 `type`속성에 할 수 있습니다.

1. ReCreate
    - 모든 `Pod`을 내리고 새로 `Pod`을 생성하는 전략
    - 운영중 배포가 중단되는 일이 발생
2. RollingUpdate
    - `maxUnavailable`속성으로 업데이트시 몇개의 Pod을 한번에  내릴건지 %비율로 설정할 수 있다.
    - `maxSurge`속성으로 업데이트시 몇개의 Pod을 한번에 생성할건지 %비율로 설정할 수 있다.
    - 무중단 배포가 가능해진다. 하지만, 그만큼 자원을 사용하기 때문에 200%이상의 자원을 더 사용하게 될수도 있다.

![이미지](https://github.com/SubiYoon/SubiYoon.github.io/blob/main/Attached%20File/스크린샷%202025-03-11%20오전%2012.03.26.png?raw=true)

---

## Service
### 기본 개념
`Service`는 `Pod`과 외부와의 연결을 위해 존재하는 Object입니다.
`Node`에 특정 `port`를 통해 `Pod`내부에 있는 Container에 접근이 가능하도록 도와주는 것이죠.

### type
`Service`에 종류에는 두가지가 있습니다.
이전 예제에서 만들어 보았던 `NodePort`방식과 default로 설정되어 있는 `ClusterIp`방식이 있습니다.

#### NodePort(검은색 화살표 진행방향)
`NodePort` 방식의 속성들을 살펴보겠습니다.

1. targetPort
    - `targetPort`란 최종적으로 Container에서 어디로 갈지 목적지 포트를 알려주는 겁니다.
2. nodePort
    - `nodePort`는 `Node`레벨에서 사용하는 포트입니다.
    - 외부에서 접근하는 포트이고 이 포트로 접속하면 `selector`로 선택한 `Pod`의 `targetPort`로 연결이 되는 것이지요

```yaml title:"Service.yaml"
ports:
  protocol: TCP
  targetPort: 8080
  nodePort: 31231
```

#### ClusterIp(초록색 화살표 진행방향)
이 타입은 `Kubernetes` 내부에서 `Pod`에 접근하기 위해서만 사용하는 타입입니다.
다시말해 내부통신을 위한 방법인것이죠.

또한 이 방식은 내부적으로 DNS를 이용한 `Service이름으로` API호출이 가능하게 됩니다.

1. `Pod`내부에서 내 자신을 호출하는 방법
```bash
# ${Service NAME}:${port}/${path}
curl http://api-teseter-1231:80/version
```

2. `Pod`내부에서 다른 `Pod`을 호출하는 방법
```bash
# ${Service NAME}.${Service NAMESPACE}:${port}/${path}
curl http://api-teseter-1231.anotherclass-123:80/version
```

`ClusterIp`방식의 속성들을 살펴보겠습니다.

1. targetPort
    - `targetPort`란 최종적으로 `Container`에서 어디로 갈지 목적지 포트를 알려주는 겁니다.
2. port
    - 내부에서 접근할 포트

```yaml title:"Service.yaml"
ports:
  protocol: TCP
  targetPort: 8080
  port: 80
```

### 포트를 명시적으로 지정
#### name
`Pod`의 속성중에 `containers - ports`하위 속성에서 설정하면 명시적으로도 설정이 가능합니다.
이 방법은 `name`을 지정해 마치 `labels`와 `selector`처럼 target을 지칭하는 것 정도로 이해해도 괜찮습니다.

이 때, `Service` Object에는 `targetPort`속성에 값을 `Pod`의 `containers - ports - name`과 일치시켜 주면 됩니다.

현재 예시가 `http`라 프로토콜 관련이라고 이해하시는 분들이 계신데 헷갈리지 마시고 그냥 고유 이름을 가르킨다고 생각하고 이해하시기 바랍니다.

#### containerPort
`name`와 같은 레벨에 있는 속성으로써 명시적으로 지칭한 `targetPort`에서 어떤 `Conainter`의 포트로 API요청을 보내야하는지를 설정하는 속성입니다.

![이미지](https://github.com/SubiYoon/SubiYoon.github.io/blob/main/Attached%20File/스크린샷%202025-03-11%20오전%2012.48.15.png?raw=true)

---

## HPA
`HPA`는 `Pod`의 갯수를 자동으로 증가/감소 시켜주는 역할을 하는 Object입니다.

### metrics
`metrics`는 어떤 상황일 때, scaleOut을 할지 정해주는 속성입니다. 아래 예시로 설명해 보겠습니다.

#### name
자원중에 어떤 자원으로 판별할지 이름을 적는 부분입니다. 우리는 `cpu`로 해보겠습니다.

#### target
 cpu의 평균 사용량이 60%가 넘으면 scaleOut을 발생하게 합니다.


```yaml
metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averegeUtilization: 60
```

### behavior
#### scaleUp
위에 작성한 `scaleOut`상황에서 좀 더 디테일하게 설정해주는 부분입니다.

아래 예시는 2분동안 60%이상의 cpu사용량을 유지할 시에 `scaleOut`을 한다는 의미입니다.

#### scaleDown
부하 감소했을 경우 `scaleIn`상황시에 좀 더 디테일하게 설정해주는 부분입니다.

아래 예시는 부하가 감소해도 10분 동안은 `Pod`의 갯수를 감소시키지 않는 다는 설정입니다.

#### policies
부하 감소시에 `Pod`의 갯수 감소에 대한 정칙을 설정하는 부분입니다.

아래 예시는 부하가 감소했을 경우 `Pod`을 1분마다 1개씩 제거한다는 의미입니다.


```yaml title:"HPA.yaml"
behavior:
  scaleUp:
    stabilizationWindowSeconds: 120
  scaleDown:
    stabilizationWindowSeconds: 600
  policies:
    - type: Pods
      value: 1
      periodSeconds: 60
```

![아마자](https://github.com/SubiYoon/SubiYoon.github.io/blob/main/Attached%20File/스크린샷%202025-03-11%20오전%201.10.51.png?raw=true)
