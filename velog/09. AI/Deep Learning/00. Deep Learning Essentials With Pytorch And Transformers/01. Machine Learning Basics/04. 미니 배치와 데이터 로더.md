---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
---
# 미니 배치와 데이터 로더

## 01. 미니 배치와 배치크기(Mini Batch and Batch Size)

```python
x_train = torch.FloatTensor([[73, 80, 75],
                            [93, 88, 93],
                            [89, 91, 90],
                            [96, 98, 100],
                            [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```

위의 데이터는 경사 하강법을 수행하여 학습할 수 있습니다.
하지만 현업에서 다루기에는 너무나 적은 양의 데이터죠. 만약 테이터가 수십만개 이상의 데이터라면 경사 하강법으로는 매우 느릴뿐 아니라 메모리의 한계로 뻗어버릴 수도 있습니다.

그렇기 때문에 전체 데이터를 작은 단위로 나누어서 해당 단위로 학습하는 개념이 나오게 되었는데, 이 단위를 `미니 배치(Mini Batch)`라고 합니다.

![[스크린샷 2026-01-19 오후 2.03.21.png]]

위의 그림은 전체 데이터를 매니 배치 단위로 나는 것을 보여줍니다.
이렇게 되면 미니 배치에 대한 비용 (cost)를 계산하고, 경사 하강법을 수행합니다. 그리고 다음 미니 배치를 가져가서 경사 하강법을 수행하는 작업을 반복하고 전체 데이터에 대한 학습이 1회 끝나면 1에포크 (Epoch)가 끝나게 되는 것입니다.

- 에포크(Epoch) : 전체 훈련 데이터가 학습에 한 번 사용된 주기

미니 배치 학습에서는 미니 배치의 개수만큼 겨사 하강법을 수행해야 전체 데이터가 한 번 전부 사용되어 1 에포크(Epoch)가 됩니다. 미니 배치의 개수는 결국 미니 배치의 크기를 몇으로 하느냐에 따라서 달라지는데 미니 배치의 크기를 `배치 크기(Batch Size)`라고 합니다.

- 배치 경사 하강법 : 전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법
- 미니 배치 경사 하강법 : 미니 배치 단위로 경사 하강법을 수행하는 방법
    - 배치의 크기는 보통 2의 제곱수를 사용 ex) 2, 4, 8, 16, ... 이유는 CPU와 GPU의 메모리가 2의 배수이므로 배치크기가 2의 제곱수일 경우에 데이터 송수신의 효율을 높일 수 있다고 합니다.

## 02. 이터레이션(Iteration)
아래 그림은 에포크와 배치 크기와 이터레이션의 관계를 보여줍니다.
즉, 이터레이션은 배치 사이즈로 나누었을 경우 전체 데이터 대비 몇변 반복하는지에 대한 횟수입니다.

![[스크린샷 2026-01-19 오후 2.27.21.png]]

## 03. 데이터 로드하기(Data Load)
파이토치에서는 데이터를 좀 더 다루기 쉽게 유용한 도구로서 `데이터셋(Dataset)`과 `데이터로더(DataLoader)`를 제공합니다.
이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle), 병렬 처리까지 간단히 수행할 수 있습니다.

Dataset을 커스텀해서 사용할 수도 있지만, TensorDataset을 사용해보겠습니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
```

TensorDataset은 기본적으로 텐서를 입력으로 받습니다.

```python
x_train = torch.FloatTensor([[73, 80, 75],
                            [93, 88, 93],
                            [89, 91, 90],
                            [96, 98, 100],
                            [73, 66, 70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```

이제 이를 TensorDataset의 입력으로 사용하고 dataset으로 저장합니다.

```python
dataset = TensorDataset(x_train, y_train)
```

파이토치의 데이터셋을 만들었다면 데이터로더를 사용 가능합니다.
데이터로더는 2개의 인자를 받는데, `Dataset`과 `Batch size`입니다. 추가적으로 `shuffle`이 있습니다. `shuffle`의 경우에는 데이터셋의 순서에 익숙해지는 것을 방지하기 위해 사용합니다. 되도록 `True`를 주는 것을 권장합니다.

```python
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# 모델 설계
model = nn.Linear(3, 1)

# 옵티마이저 설계
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)
```

이제 훈련을 시켜봅시다.

```python
nb_epochs = 20
for epoch in range(nb_epochs + 1):
    for batch_idx, samples in enumerate(dataloader):
        # print(batch_idx)
        # print(samples)
        x_train, y_train = samples
        
        prediction = model(x_train)
        
        cost = F.mse_loss(prediction, y_train)
        
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        
        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(epoch, nb_epochs, batch_idx+1, len(dataloader),cost.item()))
```

```bash
Epoch 0/20 Batch 1/3 Cost: 26085.919922
Epoch 0/20 Batch 2/3 Cost: 3660.022949
Epoch 0/20 Batch 3/3 Cost: 2922.390869
... 중 략...
Epoch 20/20 Batch 1/3 Cost: 6.315856
Epoch 20/20 Batch 2/3 Cost: 13.519956
Epoch 20/20 Batch 3/3 Cost: 4.262849
```

이제 한번 예측값을 뽑아볼까요??

```python
new_var = torch.FloatTensor([[73, 80, 75]])
pred_y = model(new_var)
print(print(" 훈 련 후 입 력 이 73, 80, 75 일 때 의 예 측 값 : ", pred_y))
```

```bash
훈 련 후 입 력 이 73, 80, 75 일 때 의 예 측 값 :  tensor([[153.9945]], grad_fn=<AddmmBackward0>)
```


## 04. 커스텀 데이터셋(Custom Dataset)
`torch.utils.data.Dataset`을 상속받아 직접 커스텀 데이터셋(Custom Dataset)을 만드는 경우도 있습니다. `torch.utils.data.Dataset`은 파이토치에서 데이터셋을 제공하는 추상 클래스입니다.

다음과 같이 기본적인 define을 정의해야 합니다.

```python
class CustomDataset(torch.utils.data.Dataset):
    # 데이터셋의 전처리 - 생성자 호출시 동작하는 로직
    def __init__(self):
    
    # 데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분
    def __len__(self):
    
    # 데이터셋에서 특정 1개의 샘플을 가져오는 함수
    def __getitem__(self, idx):
```

## 05. 커스텀 데이터셋(Custom Dataset)으로 선형 회귀 구현하기
커스텀 데이터셋(Custom Dataset) 클래스를 생서해보겠습니다.
인자를 직접 받아도 되지만, 우선 하드코딩으로 작성해보죠.

```python
import torch
import torch.nn.functional as F
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self):
        self.x_data = torch.FloatTensor([[73, 80, 75],
                            [93, 88, 93],
                            [89, 91, 90],
                            [96, 98, 100],
                            [73, 66, 70]])
        self.y_data = torch.FloatTensor([[152], [185], [180], [196], [142]])
    
    def __len__(self):
        return len(self.x_data)

    def __getitem__(self, idx):
        x = torch.FloatTensor(self.x_data[idx])
        y = torch.FloatTensor(self.y_data[idx])
        return x, y
```

이제 데이터셋을 생성하고 `DataLoader`를 통해 배치 크기를 설정하고 데이터를 무작위로 섞을지 여부를 결정합니다.

또한, 입력 차원이 3이고, 출력 차원이 1인 선형 회귀모델을 정의하고 옵티마이저를 정의합니다

```python
dataset = CustomDataset()
train_loader = DataLoader(dataset=dataset, batch_size=2, shuffle=True)

model = torch.nn.Linear(3, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)
```

이제 학습을 시작하겠습니다.

```python
nb_epochs = 20
for epoch in range(nb_epochs + 1):
    for batch_idx, samples in enumerate(data_loader):
        x_train, y_train = samples
        prediction = model(x_train)
        cost = F.mse_loss(prediction, y_train)
        optimizer.zero_grad()
        cost.backward()
        optimizer.step()
        print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(
            epoch, nb_epochs, batch_idx+1, len(data_loader), cost.item()
        ))
```

```bash
Epoch 0/20 Batch 1/3 Cost: 29410.156250
Epoch 0/20 Batch 2/3 Cost: 7150.685059
Epoch 0/20 Batch 3/3 Cost: 3482.803467
... 중 략...
Epoch 20/20 Batch 1/3 Cost: 0.350531
Epoch 20/20 Batch 2/3 Cost: 0.653316
Epoch 20/20 Batch 3/3 Cost: 0.010318
```

이제 임의의 값을 입력하여 예측값을 출력해보겠습니다.

```python
new_var = torch.FloatTensor([[73, 80, 75]])
pred_y = model(new_var)
print(" 훈 련 후 입 력 이 73, 80, 75 일 때 의 예 측 값 :", pred_y)
```

```bash
훈 련 후 입 력 이 73, 80, 75 일 때 의 예 측 값 : tensor([[151.2319]], grad_fn=<AddmmBackward>)
```