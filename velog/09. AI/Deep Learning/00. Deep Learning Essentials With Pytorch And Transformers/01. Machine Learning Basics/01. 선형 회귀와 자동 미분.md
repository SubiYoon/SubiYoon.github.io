---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
  - 선형회귀
---
# 선형 회귀와 자동 미분

## 01. 데이터에 대한 이해(Data Definition)

### 훈련 데이터셋과 테스트 데이터셋
모델을 학습 시키기 위한 데이터는 파이토의 텐서의 형태(torch.tensor)를 가지고 있어야 합니다.
입력과 출력을 각기 다른 텐서에 저장할 필요가 있습니다.
이때, 보편적으로 입력은 x, 출력은 y를 사용하여 표기합니다.

```python
import torch

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```


## 02. 가설(Hypothesis)수립
머신 러닝에서 식을 세울때 이 식을 가설(Hypothesis)라고 합니다.
가설은 임의의 추측과 경험 등으로 세울 수 있습니다. 맞는 가설이 아니라고 판단시 계속 수정해나가게 되는 식이기도 합니다.

### 선형회귀
선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다.
이때, 선형 회귀의 가설(직선의 방정싱)은 아래와 같은 형식을 가집니다.

$$
y= Wx+b
$$

가설 $H$를 따서 $y$대신 다음과 같이 식을 표현하기도 합니다.

$$
H(x) = Wx+b
$$

이때, $x$와 곱해지는 $W$를 가중치(Weight)라고 하며, $b$를 편향(bias)이라고 합니다.


- $W$와 $b$는 중학교 수학 과정인 직선의 방정식에서 기울기와 y절편에 해당됩니다.


## 03. 비용 함수(Cost function)에 대한 이해
앞으로 이런 용어들을 본다면 전부 같은 용어로 생각하면 됩니다.
**비용 함수(cost function) == 손실함수(loss function) == 오차함수(error fucntion) == 목적함수(objective function)**


특히, 비용 함수와 손실 함수란 용어는 기억해주는 것이 좋습니다.


### 평균 제곱 오차(Mean Squared Error, MSE)
각 오차들을 제곱해서 더하여 n으로 나눠주면 오차의 제곱합에 대한 합의 평균을 구할 수 있습니다.

$$
{1 \over n}\sum\limits_{i=1}^{n}[y^{(i)} - H(x^{i})]^2
$$

**평균 제곱 오차**는 회귀 문제에서 적벌한 $W$와 $b$를 찾기 위한 최적화된 식입니다.
그 이유는 평균 제곱 오차의 최소값으로 만드는 $W$와 $b$를 찾아내는 것이 가장 훈련 테이터를 잘 반영한 직선을 찾아내는 일이기 때문입니다.


평균 제곱 오차를 $W$와 $b$에 의한 비용 함수(Cost function)로 재정의해보면 다음과 같습니다.

$$
cost(W, b) = {1 \over n}\sum\limits_{i=1}^{n}[y^{(i)} - H(x^{i})]^2
$$

따라서 $cost(W, b)$를 최소로 갖는 $W$와 $b$를 구하면 훈련 데이터를 가장 잘 나타내는 직선을 구할 수 있는 겁니다.

## 04. 옵티마이저 - 경사  하강법(Gradient Descent)
앞서 배운 비용 함수(Cost fucntion)의 값을 최소로하는 $W$와 $b$를 찾을 때 사용되는 것이 **옵티머이저(Optimizer)** 알고리즘입니다. 최적화 알고리즘이라고도 부릅니다.
이 옵티마이저 알고리즘을 통해. 적절한 $W$와 $b$를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 합니다.

### 경사하강법(Gradient Descent)
가장 기본적인 옵티마이저 알고리즘입니다.


생각을 해봅시다.


위에서 언급한 $y = Wx + b$에서 기울기가 커지면 커질수록 $cost$의 값은 무한대로 커질겁니다.
반대로 기울기가 작아지면 작아질 수록 $cost$의 값은 무한대로 커질겁니다.


$W$와 $cost$의 관계 그래프를 그려보면 다음과 같은 그래프가 나올 겁니다.


![[스크린샷 2026-01-14 오후 3.08.03.png]]


 이제 위의 그래프에서 우리가 찾아야할건 아래로 볼록한 부분중에 $cost$가 가장 최소가 되는 부분입니다. 그리고 기계가 해야할 일은 $cost$가 가장 최소값을 가지게 하는 $W$를 찾는 일입니다.


임의의 초기값 $W$를 설정하고 $cost$의 최솟값을 찾을 때 까지 내려가는 것이지요. 이를 가능하게 하는 것이 아래와 같은 **경사 하강법(Gradient Descent)** 입니다.


![[스크린샷 2026-01-14 오후 3.13.31.png]]

또한 이 과정에서 기울기의 개념을 사용하는데 접선의 기울기가 0인 부분을 찾는 것이죠.
즉, $cost$가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 이 식에서 미분값이 0이 되는 지점인 것이죠.


![[스크린샷 2026-01-14 오후 3.13.53.png]]


이 반복 작업에는 현재 $W$의 접선의 기울기 (gradient)를 구해 특정 숫자 $\alpha$를 곱한 값을 빼서 새로운 $W$로 사용하는 식이 사용 됩니다.

$$
gradient = {{\partial cost (W)} \over {\partial W}}
$$

- 기울기가 음수일 때 (Negative gradient) : $W$의 값이 증가

$$
W \coloneqq W - \alpha \times (-gradient) = W + \alpha \times gradient
$$

기울기가 음수면 $W$의 값이 증가하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정됩니다.


- 기울기가 양수일 때(Positive gradient) : $W$의 값이 감소

$$
W \coloneqq W - \alpha \times (+gradient)
$$

기울기가 양수면 $W$의 값이 감소하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정됩니다.


아래 식은 기울기 값이 음수인지 양수인지 상관없이 기울기가 0인 방향으로 $W$를 조정합니다.

$$
W \coloneqq W - \alpha {{\partial} \over {\partial W}}cost(W)
$$


여기서 $\alpha$는 학습률(learning rate)를 뜻합니다.

### 학습률(learning rate)
**학습률 $\alpha$** 은 $W$의 값을 변경할 때, 얼마나 크게 변경할지를 결정합니다.
직관적으로 생각하기에 $\alpha$를 무작정 크게 하면 접선의 기울기가 최소가 되는 $W$를 빠르게 찾을 수 있을거 같지만 그렇지 않습니다.

![[스크린샷 2026-01-14 오후 3.41.09.png]]


위의 그림은 **학습룰 $\alpha$**가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 $W$를 찾아가는 것이 아니라 $cost$의 값이 발상하는 상황을 보여줍니다. 반대로 **학습률 $\alpha$** 가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 $\alpha$를 찾는 것도 중요합니다.


지금 우리가 다뤄본 케이스는 $b$는 배제시키고 최적의 $W$를 찾아내는 것에만 초점을 맞추었습니다.
실제 경사 하강법은 $W$와 $b$에 대해서 동시에 경사 하강법을 수행하면서 최적의 $W$와 $b$의 값을 찾아갑니다.

- 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적인 개념
- 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있음
- 션형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법

## 05. 파이토치로 선형 회귀 구현하기

### 기본 셋팅
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1) # 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.
```


### 변수 선언

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```

### 가중치와 편향 초기값
선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 입니다.
그것을 결정하는 것은 $W$와 $b$의 값이죠.
선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 $W$와 $b$의 값을 찾는 것이죠.

우선 가중치 $W$를 0으로 초기화 하겠습니다.

```python
W = torch.zeros(1, requires_grad=True)
print(W)
```

편향 $b$도 0으로 초기화 하겠습니다.

```python
b = torch.zeros(1, requires_grad=True)
print(b)
```

현재 가중치는 $W$와 $b$둘다 0이므로 현 직선의 방정식은 다음과 같습니다.

$$
y = 0 \times x + 0
$$

지금 상태에서는 $x$에 어떤 값이 와도 가설은 0을 예측하므로 적절한 $W$와 $b$가 아닙니다.

### 가설 세우기
파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언합니다.
$$
H(x) = Wx + b
$$

```python
hypothesis = x_train * W + b
```

### 비용 함수 선언하기
파이토치 코드 상으로 선형 회귀의 비용 함수에 해다하는 평균 제곱 오차를 선언합니다.

$$
cost(W, b) = {1 \over n}\sum\limits_{i=1}^{n}[y^{(i)} - H(x^{i})]^2
$$

```python
cost = torch.mean((hypothesis - y_train) ** 2)
```

### 경사 하강법 구현하기
아래의 `SGD`는 경사하강법의 일종입니다.
`lr`은 학습률(learning rate)를 의미합니다.

```python
optimizer = optim.SGD([W, b], lr=0.01)
```

`optimizer.zero_grad()`를 실행해 미분을 통해 얻은 기울기를 0으로 초기화 합니다.
기울기를 초기화 해야만 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있습니다.
`cost.backward()`를 호출하면 가중치 $W$와 편향 $b$에 대한 기울기가 계산 됩니다.
그 다음 경사하강법 최적화 함수 `optimizer.step()`를 호출하여 인수로 들어갔던 $W$와 $b$에서 리턴되는 변수들의 기울기에 학습률(learning rate) 0.01을 곱하여 빼줌으로서 업데이트 합니다.ㅁ

### 전체 코드
```python
# 데 이 터
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])

# 모 델 초 기 화
W = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# optimizer 설 정
optimizer = optim.SGD([W, b], lr=0.01)
nb_epochs = 2000 # 원 하 는 만 큼 경 사 하 강 법 을 반 복
for epoch in range(nb_epochs + 1):
	# H(x) 계 산
	hypothesis = x_train * W + b
	
	# cost 계 산
	cost = torch.mean((hypothesis - y_train) ** 2)
	
	# cost 로 H(x) 개 선
	optimizer.zero_grad()
	cost.backward()
	optimizer.step()
	
	# 100 번 마 다 로 그 출 력
	if (epoch + 1) % 100 == 0:
		print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
		    epoch + 1, nb_epochs, W.item(), b.item(), cost.item()
		))
```

```bash
Epoch 100/2000 W: 1.746, b: 0.578 Cost: 0.048171
Epoch 200/2000 W: 1.800, b: 0.454 Cost: 0.029767
Epoch 300/2000 W: 1.843, b: 0.357 Cost: 0.018394
Epoch 400/2000 W: 1.876, b: 0.281 Cost: 0.011366
Epoch 500/2000 W: 1.903, b: 0.221 Cost: 0.007024
Epoch 600/2000 W: 1.924, b: 0.174 Cost: 0.004340
Epoch 700/2000 W: 1.940, b: 0.136 Cost: 0.002682
Epoch 800/2000 W: 1.953, b: 0.107 Cost: 0.001657
Epoch 900/2000 W: 1.963, b: 0.084 Cost: 0.001024
Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633
Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391
Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242
Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149
Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092
Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057
Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035
Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022
Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013
Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008
Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005
```


**에포크(Epoch)**는 전체 훈련 데이터가 학습에 한번 사용된 주기를 말하니다.
이번 실습의 경우 2000번을 수행한 경우 입니다.
최중 훈련 결과를 보면 $W$는 2에 수렴하고 $b$는 0에 수렴하는 것을 볼 수 있습니다.
$H(x) = 2x$이므로 정답을 찾은 것입니다.


### optimizer.zero_grad()가 필요한 이유
파이토치는 미분을 통해 얻은 기울기를 이전에 계산된 기울기에 누적합산하는 특징이 있습니다.

```python
import torch

w = torch.tensor(2.0, requires_grad=True)

nb_epochs = 20

for epoch in range(nb_epochs + 1):
	z = 2*w
	z.backward()
	print(f'수식을 w로 미분한 값 : {w.grad}')
```

```bash
수 식 을 w 로 미 분 한 값 : 2.0
수 식 을 w 로 미 분 한 값 : 4.0
수 식 을 w 로 미 분 한 값 : 6.0
수 식 을 w 로 미 분 한 값 : 8.0
수 식 을 w 로 미 분 한 값 : 10.0
수 식 을 w 로 미 분 한 값 : 12.0
수 식 을 w 로 미 분 한 값 : 14.0
수 식 을 w 로 미 분 한 값 : 16.0
수 식 을 w 로 미 분 한 값 : 18.0
수 식 을 w 로 미 분 한 값 : 20.0
수 식 을 w 로 미 분 한 값 : 22.0
수 식 을 w 로 미 분 한 값 : 24.0
수 식 을 w 로 미 분 한 값 : 26.0
수 식 을 w 로 미 분 한 값 : 28.0
수 식 을 w 로 미 분 한 값 : 30.0
수 식 을 w 로 미 분 한 값 : 32.0
수 식 을 w 로 미 분 한 값 : 34.0
수 식 을 w 로 미 분 한 값 : 36.0
수 식 을 w 로 미 분 한 값 : 38.0
수 식 을 w 로 미 분 한 값 : 40.0
수 식 을 w 로 미 분 한 값 : 42.0
```

계속해서 미분값인 2를 더하는게 보이시죠??
이러한 특징 때문에 새로운 기울기를 받아들일 때, 초기화를 시켜주어야 하는 겁니다.
그리고 그 기능을 `zero.grad()`가 하는 것이죠.


### torch.manual_seed()를 하는 이유
`torch.manual_seed()`를 사용한 프로그램의 결과는 다른 컴퓨터에서 실행시켜도 동일한 결과를 얻을 수 있습니다.
그 이유는 `torch.manual_seed()`는 난수 발생 순서와 값을 동일하게 보장해주는 특징때문입니다.

아래와 같이 `manual_seed(3)`과 `manual_seed(5)`는 값이 다르지만, 똑같이 `manual_seed(3)`를 실행하면 같은 값을 나타내는 것을 알 수 있습니다.


```python
import torch

torch.manual_seed(3)
print('랜덤 시드가 3일 때')
for i in range(1, 3):
    print(torch.rand(1))

torch.manual_seed(5)
print('랜덤 시드가 5일 때')
for i in range(1, 3):
    print(torch.rand(1))

torch.manual_seed(3)
print('랜덤 시드가 3일 때')
for i in range(1, 3):
    print(torch.rand(1))
```

```bash
랜덤 시드가 3일 때
tensor([0.0043])
tensor([0.1056])

랜덤 시드가 5일 때
tensor([0.8303])
tensor([0.1261])

랜덤 시드가 3일 때
tensor([0.0043])
tensor([0.1056])
```


텐서에는 `requires_grad`라는 속성이 있습니다. 이것을 `True`로 설정하면 자동미분 기능이 적용됩니다. 선형 회구부터 신경망과 같은 복잡한 구조에서 파라미터들이 모두 이 기능이 적용됩니다. `requires_gard=True`가 적용된 텐서에 연산을 하면, 계산 그래프가 생성되며 `backward()`를 호출시 그래프로부터 자동으로 미분이 계산됩니다.


### 8. 자동 미분(Autograd) 실습하기
임의로 $2w^2 + 5$라는 식을 세워보고 $w$에 대해 미분해보겠습니다.

```python
import torch

w = torch.tensor(2.0, requires_grad=True)

z = 2*(w**2) + 5

# w에 대한 기울기름 검사
z.backward()

print(f'수식을 w로 미분한 값 : {w.grad}')
```

```bash
수 식 을 w 로 미 분 한 값 : 8.0
```