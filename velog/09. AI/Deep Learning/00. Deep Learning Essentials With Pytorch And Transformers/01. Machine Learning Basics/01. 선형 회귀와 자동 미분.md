---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
---

# 선형 회귀와 자동 미분

## 01. 데이터에 대한 이해(Data Definition)

### 훈련 데이터셋과 테스트 데이터셋
모델을 학습 시키기 위한 데이터는 파이토의 텐서의 형태(torch.tensor)를 가지고 있어야 합니다.
입력과 출력을 각기 다른 텐서에 저장할 필요가 있습니다.
이때, 보편적으로 입력은 x, 출력은 y를 사용하여 표기합니다.

```python
import torch

x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```


## 02. 가설(Hypothesis)수립
머신 러닝에서 식을 세울때 이 식을 가설(Hypothesis)라고 합니다.
가설은 임의의 추측과 경험 등으로 세울 수 있습니다. 맞는 가설이 아니라고 판단시 계속 수정해나가게 되는 식이기도 합니다.

### 선형회귀
선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일입니다.
이때, 선형 회귀의 가설(직선의 방정싱)은 아래와 같은 형식을 가집니다.

$$
y= Wx+b
$$

가설 $H$를 따서 $y$대신 다음과 같이 식을 표현하기도 합니다.

$$
H(x) = Wx+b
$$

이때, $x$와 곱해지는 $W$를 가중치(Weight)라고 하며, $b$를 편향(bias)이라고 합니다.


- $W$와 $b$는 중학교 수학 과정인 직선의 방정식에서 기울기와 y절편에 해당됩니다.


## 03. 비용 함수(Cost function)에 대한 이해
앞으로 이런 용어들을 본다면 전부 같은 용어로 생각하면 됩니다.
**비용 함수(cost function) == 손실함수(loss function) == 오차함수(error fucntion) == 목적함수(objective function)**


특히, 비용 함수와 손실 함수란 용어는 기억해주는 것이 좋습니다.


### 평균 제곱 오차(Mean Squared Error, MSE)
각 오차들을 제곱해서 더하여 n으로 나눠주면 오차의 제곱합에 대한 합의 평균을 구할 수 있습니다.

$$
{1 \over n}\sum\limits_{i=1}^{n}[y^{(i)} - H(x^{i})]^2
$$

**평균 제곱 오차**는 회귀 문제에서 적벌한 $W$와 $b$를 찾기 위한 최적화된 식입니다.
그 이유는 평균 제곱 오차의 최소값으로 만드는 $W$와 $b$를 찾아내는 것이 가장 훈련 테이터를 잘 반영한 직선을 찾아내는 일이기 때문입니다.


평균 제곱 오차를 $W$와 $b$에 의한 비용 함수(Cost function)로 재정의해보면 다음과 같습니다.

$$
cost(W, b) = {1 \over n}\sum\limits_{i=1}^{n}[y^{(i)} - H(x^{i})]^2
$$

따라서 $cost(W, b)$를 최소로 갖는 $W$와 $b$를 구하면 훈련 데이터를 가장 잘 나타내는 직선을 구할 수 있는 겁니다.

## 04. 옵티마이저 - 경사  하강법(Gradient Descent)
앞서 배운 비용 함수(Cost fucntion)의 값을 최소로하는 $W$와 $b$를 찾을 때 사용되는 것이 **옵티머이저(Optimizer)** 알고리즘입니다. 최적화 알고리즘이라고도 부릅니다.
이 옵티마이저 알고리즘을 통해. 적절한 $W$와 $b$를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 합니다.

### 경사하강법(Gradient Descent)
가장 기본적인 옵티마이저 알고리즘입니다.


생각을 해봅시다.


위에서 언급한 $y = Wx + b$에서 기울기가 커지면 커질수록 $cost$의 값은 무한대로 커질겁니다.
반대로 기울기가 작아지면 작아질 수록 $cost$의 값은 무한대로 커질겁니다.


$W$와 $cost$의 관계 그래프를 그려보면 다음과 같은 그래프가 나올 겁니다.


![[스크린샷 2026-01-14 오후 3.08.03.png]]


 이제 위의 그래프에서 우리가 찾아야할건 아래로 볼록한 부분중에 $cost$가 가장 최소가 되는 부분입니다. 그리고 기계가 해야할 일은 $cost$가 가장 최소값을 가지게 하는 $W$를 찾는 일입니다.


임의의 초기값 $W$를 설정하고 $cost$의 최솟값을 찾을 때 까지 내려가는 것이지요. 이를 가능하게 하는 것이 아래와 같은 **경사 하강법(Gradient Descent)** 입니다.


![[스크린샷 2026-01-14 오후 3.13.31.png]]

또한 이 과정에서 기울기의 개념을 사용하는데 접선의 기울기가 0인 부분을 찾는 것이죠.
즉, $cost$가 최소화가 되는 지점은 접선의 기울기가 0이 되는 지점이며, 이 식에서 미분값이 0이 되는 지점인 것이죠.


![[스크린샷 2026-01-14 오후 3.13.53.png]]


이 반복 작업에는 현재 $W$의 접선의 기울기 (gradient)를 구해 특정 숫자 $\alpha$를 곱한 값을 빼서 새로운 $W$로 사용하는 식이 사용 됩니다.

$$
gradient = {{\partial cost (W)} \over {\partial W}}
$$

- 기울기가 음수일 때 (Negative gradient) : $W$의 값이 증가

$$
W \coloneqq W - \alpha \times (-gradient) = W + \alpha \times gradient
$$

기울기가 음수면 $W$의 값이 증가하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정됩니다.


- 기울기가 양수일 때(Positive gradient) : $W$의 값이 감소

$$
W \coloneqq W - \alpha \times (+gradient)
$$

기울기가 양수면 $W$의 값이 감소하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W의 값이 조정됩니다.


아래 식은 기울기 값이 음수인지 양수인지 상관없이 기울기가 0인 방향으로 $W$를 조정합니다.

$$
W \coloneqq W - \alpha {{\partial} \over {\partial W}}cost(W)
$$


여기서 $\alpha$는 학습률(learning rate)를 뜻합니다.

### 학습률(learning rate)
**학습률 $\alpha$** 은 $W$의 값을 변경할 때, 얼마나 크게 변경할지를 결정합니다.
직관적으로 생각하기에 $\alpha$를 무작정 크게 하면 접선의 기울기가 최소가 되는 $W$를 빠르게 찾을 수 있을거 같지만 그렇지 않습니다.

![[스크린샷 2026-01-14 오후 3.41.09.png]]


위의 그림은 **학습룰 $\alpha$**가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 $W$를 찾아가는 것이 아니라 $cost$의 값이 발상하는 상황을 보여줍니다. 반대로 **학습률 $\alpha$** 가 지나치게 낮은 값을 가지면 학습 속도가 느려지므로 적당한 $\alpha$를 찾는 것도 중요합니다.


지금 우리가 다뤄본 케이스는 $b$는 배제시키고 최적의 $W$를 찾아내는 것에만 초점을 맞추었습니다.
실제 경사 하강법은 $W$와 $b$에 대해서 동시에 경사 하강법을 수행하면서 최적의 $W$와 $b$의 값을 찾아갑니다.

- 가설, 비용 함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적인 개념
- 풀고자하는 각 문제에 따라 가설, 비용 함수, 옵티마이저는 전부 다를 수 있음
- 션형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법

## 05. 파이토치로 선형 회귀 구현하기

### 기본 셋팅
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1) # 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 줍니다.
```


### 변수 선언

```python
x_train = torch.FloatTensor([[1], [2], [3]])
y_train = torch.FloatTensor([[2], [4], [6]])
```

### 가중치와 편향 초기값
선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 입니다.
그것을 결정하는 것은 $W$와 $b$의 값이죠.
선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 $W$와 $b$의 값을 찾는 것이죠.

우선 가중치 $W$를 0으로 초기화 하겠습니다.

```python
W = torch.zeros(1, requires_grad=True)
print(W)
```

편향 $b$도 0으로 초기화 하겠습니다.

```python
b = torch.zeros(1, requires_grad=True)
print(b)
```
