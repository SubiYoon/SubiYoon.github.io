---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
  - 다중선형회귀
---
# 다중 선형 회귀
$x$가 1개인 선형 회귀를 단순 선형 회귀(Single Linear Regression)이라고 합니다.
그러면 다수인 $x$로 부터 $y$를 예측하는 다중 선형 회귀(Multivariable Linear Regresstion)을 살펴보겠습니다.

### 데이터에 대한 이해 (Data Definition)
독립 변수 $x$의 갯수가 4개인 퀴즈 점수로부터 최종 점수를 예측하는 모델을 만들어 보겠습니다.

![[스크린샷 2026-01-15 오후 3.53.35.png]]

독립변수 $x$의 개수가 3개 이므로 이를 수식으로 표현하면 아래와 같습니다.
$$
H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b
$$

### 파이토치로 구현하기
우선 필요한 도구들을 임포트하고 랜덤 시드를 고정합니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
```


$$
H(x) = w_1x_1 + w_2x_2 + w_3x_3 + b
$$

위 식에 대한 $x$를 3개 선언합니다.

> 여기서 1차원이 2차원으로 선언하는 이유는 통상 2차원이 더 안전하고 표준적이기 때문입니다.
> 1차원으로 선언해서 사용해도 문제는 없지만 Broadcasting혼란이 야기될 수 있고, 행렬 곱셈이 불가능한 상황이 발생할 수 있으며, Shape불일치의 에러가 발생할 수 있습니다.

```python
x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])
x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])
x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])
```

이제 가중치 $w$와 편향 $b$를 선언해 보겠습니다.
$x$가 3개였으니 가중치도 3개를 선언해주어야합니다.

```python
w1 = torch.zeros(1, requires_grad=True)
w2 = torch.zeros(1, requires_grad=True)
w3 = torch.zeros(1, requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

이제 가설, 비용 함수, 옵티마이저를 선언한 후에 경사 하강법을 n회 반복해보겠습니다.

```python
# optimizer 설정
optimizer = optim.SGD([w1, w2, w3, b], lr = 1e-5)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):
    # H(x)계산
    hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 * + b
    
    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)
    
    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    # 100 번 마 다 로 그 출 력
	if epoch % 100 == 0:
		print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost:{:.6f}'.format(
            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()
        ))
```


### 벡터와 행렬 연산으로 바꾸기
위의 코드는 개선할 여지가 있습니다. $x$의 개수가 3개였으니까 x1, x2, x3의 훈련데이터들을 일일히 선언했습니다. 그런데 만약 $x$의 개수가 500개 아니 1000개라고 생각해 봅시다. 그럴때마다 위와 같은 방식으로 모두 선언하기에는 비효율적이죠.

이를 해결하기 위해 행렬 곱셈 연상(또는 벡터의 내적)을 사용합니다.
- 행렬의 곱셈 과정에서 이루어지는 벡터 연산을 벡터의 내적(Dot Product)이라고 합니다.

![[스크린샷 2026-01-16 오전 11.14.55.png]]

위의 그림은 행렬 곱셈 연산 과정에서 벡터의 내적으로 1 × 7 + 2 × 9 + 3 × 11 = 58 이 되는 과정을 보여줍니다.

#### 벡터 연산으로 이해하기
$$
H(X) = w_1x_1 + w_2x_2 + w_3x3
$$

위 식은 아래와 같이 두 벡터의 내적으로 표현할 수 있습니다.

$$
\begin{pmatrix}
x_{1} & x_{2} & x_{3}
\end{pmatrix}
\space \cdot \space
\begin{pmatrix}
w_{1} \\
w_{2} \\
w_{3}
\end{pmatrix}
=
\begin{pmatrix}
x_{1}w_{1} + x_{2}w_{2} + x_{3}w_{3}
\end{pmatrix}
$$

두 벡터를 각각 $X$와 $W$로 표현한다면, 가설은  다음과 같습니다.
$$
H(X) = XW
$$
$x$의 개수가 3개였음에도 이제는 $X$와 $Y$라는 두 개의 변수로 표현된 것을 볼 수 있습니다.

#### 행렬 연산으로 이해하기
위에서 사용했던 훈련 데이터를 살펴보겠습니다.

![[스크린샷 2026-01-15 오후 3.53.35.png]]

전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위를 샘플(sample)이라고 합니다. 현재 샘플의 수는 총 5개 입니다.
각 샘플에서 $y$를 결정하게 하는 가각의 독립변수 $x$를 특성(feature)이라고 합니다. 현재 특성은 3개 입니다.
이는 독립 변수의 수가 15(샘플의 수 $\times$ 특성의 수)개를 의미합니다.

이젠 독립 변수 $x$들을 하나의 행렬로 표현하면 아래와 같습니다. 이 행렬을 $X$라고 해보죠.

$$
X = \begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \\
x_{41} & x_{42} & x_{43} \\
x_{51} & x_{52} & x_{53}
\end{pmatrix}
$$

그리고 여기에 가중치 $w_{1}, w_{2}, w_{3}$을 원소라하는 벡터를 $W$라 하고 이를 곱해보겠습니다.

$$
\begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \\
x_{41} & x_{42} & x_{43} \\
x_{51} & x_{52} & x_{53}
\end{pmatrix}
\begin{pmatrix}
w_{1} \\
w_{2} \\
w_{3}
\end{pmatrix}
=
\begin{pmatrix}
x_{11}w_{1} & x_{12}w_{2} & x_{13}w_{3} \\
x_{21}w_{1} & x_{22}w_{2} & x_{23}w_{3} \\
x_{31}w_{1} & x_{32}w_{2} & x_{33}w_{3} \\
x_{41}w_{1} & x_{42}w_{2} & x_{43}w_{3} \\
x_{51}w_{1} & x_{52}w_{2} & x_{53}w_{3}
\end{pmatrix}
$$

위 식은 결과적으로 다음과 같습니다.

$$
H(X) = XW
$$

이 가설에 각 샘플에 더해지는 편향 $b$를 추가하면 다음과 같습니다.

$$
\begin{pmatrix}
x_{11} & x_{12} & x_{13} \\
x_{21} & x_{22} & x_{23} \\
x_{31} & x_{32} & x_{33} \\
x_{41} & x_{42} & x_{43} \\
x_{51} & x_{52} & x_{53}
\end{pmatrix}
\begin{pmatrix}
w_{1} \\
w_{2} \\
w_{3}
\end{pmatrix}
\begin{pmatrix}
b \\
b \\
b \\
b \\
b
\end{pmatrix}
=
\begin{pmatrix}
x_{11}w_{1} & x_{12}w_{2} & x_{13}w_{3} + b \\
x_{21}w_{1} & x_{22}w_{2} & x_{23}w_{3} + b \\
x_{31}w_{1} & x_{32}w_{2} & x_{33}w_{3} + b \\
x_{41}w_{1} & x_{42}w_{2} & x_{43}w_{3} + b \\
x_{51}w_{1} & x_{52}w_{2} & x_{53}w_{3} + b
\end{pmatrix}
$$

위 식은 결과적으로 다음과 같습니다.

$$
H(X) = XW + b
$$

결과적으로 가설 연산을 3개의 변수만으로 표현한 것입니다.
이렇게 벡터와 행렬 연산은 식을 간단하게 해줄 뿐 아니라 다수의 샘플의 병렬 연산이므로 속도의 이점을 가집니다.

### 행렬 연산을 고려하여 파이토치로 구현하기
이번엔 행렬 연산을 고려하여 파이토치로 재구현해보겠습니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

x_train = torch.FloatTensor([
    [73, 80, 75],
    [93, 88, 93],
    [89, 91, 80],
    [96, 98, 100],
    [73, 66, 70]
])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

```

x_train과 y_train의 shape를 출력하면 다음과 같이 나옵니다.

```bash
torch.Size([5, 3])
torch.Size([5, 1])
```

각각 $(5 \times 3)$행렬과 $(5 \times 1)$행렬의 크기를 갖습니다.
이제 가중치 $W$와 편향 $b$를 선언합니다.

여기서 주목해야하는건 $W$의 크기가 $(3 \times 1)$의 벡터라는 점입니다.
형렬의 곱셈은 좌측에 있는 `열의 크기`와 우측에 있는 `행의 크기`가 일치해야합니다.

```python
w = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

이젠 가설을 행렬곱으로 간단히 정의해 보겠습니다.

```python
hypothesis = x_train.matmul(w) + b
```

앞서 선언했던 방식과의 차이를 비교해 보겠습니다.

```python
hypothesis = x1_train * w1 + x2_train * w2 + x3_train * w3 * + b
```

확연히 차이가 나는게 보이시나요??
이렇게 훨씬 가시성이 좋게 변했씁니다.

완전한 코드는 다음과 같습니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

x_train = torch.FloatTensor([
    [73, 80, 75],
    [93, 88, 93],
    [89, 91, 80],
    [96, 98, 100],
    [73, 66, 70]
])
y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])

w = torch.zeros((3, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

# optimizer 설정
optimizer = optim.SGD([w, b], lr = 1e-5)

nb_epochs = 20
for epoch in range(nb_epochs + 1):
    # H(x)계산
    hypothesis = x_train.matmul(w) + b
    
    # cost 계산
    cost = torch.mean((hypothesis - y_train) ** 2)
    
    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    # 100 번 마 다 로 그 출 력
    print('Epoch {:4d}/{} hypothesis: {} Cost:{:.6f}'.format(
        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()
    ))
```

학습이 끝난 모델에 임의의 입력 값을 넣어 예측을 해봅시다.

```python
# 임의의 입력 값에 대한 예측
with torch.no_grad():
    # 예측하고 싶은 임의의 입력값
    new_input = torch.FloatTensor([[75, 85, 72]]) 
    
    # 예측하고 임의의 입력값에 대한 예측 값을 계산
    prediction = new_input.matmul(w) + b
    
    # 예측된 값 출력
    print('Predicted value for input {}: {}'.format(new_input.squeeze().tolist(),prediction.item()))
```

```bash
Predicted value for input [75.0, 85.0, 72.0]: 156.8051300048828
```

#### with torch.no_grad()
이 블록 안에서 수행되는 모든 연산에 대해 역전파(즉, 기울기 계산)를 비활성화 합니다.
예측시에는 가중치를 업데이트할 필요가 없겠죠?? 그렇기 때문에 메모리와 계산 자원을 절약하기 위해 `torch.no_grad()`를 사용하는 것이 좋습니다.