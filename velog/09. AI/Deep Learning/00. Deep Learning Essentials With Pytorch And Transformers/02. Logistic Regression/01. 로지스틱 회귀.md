---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
  - 로지스틱회귀
---
# 로지스틱 회귀
두 개의 선택지 중에서 정답을 고르는 문제가 많은데, 이렇게 둘 중 하나를 결정하는 문제를 `이진 분류(Binary Classification)`라고 합니다.
바로 이진 분류를 풀기 위한 대표적인 알고리즘이 로지스틱 회귀(Logistic Regression)가 있습니다.
- 로지스틱 회귀는 알고리즘의 이름은 회귀지만 실제로는 분류(Classification)작업에 사용할 수 있습니다.

## 01. 이진 분류(Binary Classification)
이런 데이터가 있다고 가정해봅시다.
특정 점수를 얻었을 때의 합격, 불합격 여부를 판정하는 모델을 만드는 것이 목적입니다

| score($x$) | result($y$) |
| ---------- | ----------- |
| 45         | 불합격         |
| 50         | 불합격         |
| 55         | 불합격         |
| 60         | 합격          |
| 65         | 합격          |
| 70         | 합격          |

위의 데이터를 기반으로 합격을 1, 불합격을 0이라고 가정하여 그래프를 그려보면 다음과 같습니다.

![[스크린샷 2026-01-19 오후 5.58.45.png]]

이러한 그래프는 $H(x) = Wx + b$의 형태로 표현할 수 없습니다. 보통 이런 그래프는 알파벳 'S'자 형태의 모양으로 표현되는데, 어떤 특정 함수 f를 추가적으로 사용하여 $H(x) = f(Wx + b)$의 가설을 사용할 겁니다. 그리고 이런 S자 모양의 그래프를 그릴 수 있는 함수가 이미 널리 알려져있습니다.
바로! 시그모이드 함수입니다.

## 02. 시그모이드 함수(Sigmoid function)
$$
H(x) = sigmoid(Wx + b) = + {{1} \over {1 + e^{-(Wx + b)}}} = \sigma(Wx + b) 
$$
$$
e(자연상수) \eqsim 2.7182818284\dots
$$

선형 회귀에서는 최적의 $W$와 $b$를 찾는 것이 목표였습니다. 여기서도 마찬가지 입니다.
선형 회귀에서는 $W$가 기울기, $b$가 $y$절편을 의미했다면, 여기서는 어떤지 알아보겠습니다.

우선 그래프를 그리기 위해. `Matplotlib`과 `Numpy`를 임포트합니다.

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt
```

Numpy를 이용하여 시그모이드 함수를 정의합니다.

```python
def sigmoid(x): # 시그모이드 함수 정의
    return 1 / (1 + np.exp(-x))
```

### $W$가 1이고 $b$가 0인 그래프

```python
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)
plt.plot(x, y, 'g')
plt.plot([0, 0], [1.0, 0.0], ':') # 가운데에 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![[스크린샷 2026-01-20 오전 10.08.09.png]]

### $W$값의 변화에 따른 경사도의 변화

$W$의 값에 따라 경사도가 달라지는 것을 볼 수 있습니다.

```python
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(0.5*x)
y2 = sigmoid(x)
y3 = sigmoid(2*x)
plt.plot(x, y1, 'r', linestyle='--') # 빨간선
plt.plot(x, y2, 'g') # 초록선, W의 값이 1일때
plt.plot(x, y3, 'b', linestyle='--') # 파란선
plt.plot([0, 0], [1.0, 0.0], ':') # 가운데에 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

![[스크린샷 2026-01-20 오전 10.44.09.png]]

### $b$값의 변화에 따른 좌, 우 이동

아래 그래프처럼 $b$값에 따라 그래프가 좌, 우로 이동하는 것을 보여줍니다.

```python
x = np.arange(-5.0, 5.0, 0.1)
y1 = sigmoid(x+0.5)
y2 = sigmoid(x+1)
y3 = sigmoid(x+1.5)
plt.plot(x, y1,'r', linestyle='--') # x + 0.5
plt.plot(x, y2,'g') # x + 1
plt.plot(x, y3,'b', linestyle='--') # x + 1.5
plt.plot([0,0],[1.0,0.0], ':') # 가 운 데 점 선 추 가
plt.title('Sigmoid Function')
plt.show()
```

![[스크린샷 2026-01-20 오전 11.14.32.png]]


### 시그모이드 함수를 이용한 분류
시그모이드 함수는 입력값이 양의 무한대로 커지면 1에 수렴하고, 입력값이 음의 무한대로 작아지면 0에 수렴합니다. 따라서, 출력값은 0과 1사이의 값을 가지는데 이 특성을 이용하여 분류작업에 사용할 수 있습니다ㅑ.

예를 들어 임계값을 0.5라고 정하고 출력값이 0.5이상이면 1(True), 0.5이하면 0(False)로 판단하도록 설계할 수 있는 것이죠.

## 03. 비용함수(Cost function)
$$
cost(W,b) = {1 \over n}\sum_{i=1}^{n}{[y^{(i)} - H(x^{(i)})]}^2
$$

위에 식은 평균 제곱 오차의 수식입니다. 함수식에서의 가설은 $H(x) = Wx - b$이지만, 이제 $H(x) = sigmoid(Wx + b)$를 사용해야합니다. 그리고 이 비용 함수를 미분하면 선형 회귀때와 달리 다음과 같이 유사한 심볼 비볼릭(non-convex) 형태의 그래프가 나옵니다.

![[스크린샷 2026-01-20 오후 1.20.56.png]]

위에 그래프를 보면 기울기값이 0인 부분이 한개가 아님을 확인 할 수 있습니다.
이 경우 문제점은 경사 하강법이 오차가 최소값이 되는 구간에 도착했다고 판단한 그 구간이 실제 오차가 완전히 최소값이 아닐 수 있다는 점입니다.

전체 함수에 걸쳐 최소값인 `글로벌 미니멈(Global Minimum)`이 아닌 특정 구역에서의 최소값인 `로컬 미니멈(Local Minimun)`으로 나눕니다. 이는 cost가 최소가 되는 가중치 $W$를 찾는다는 비용 함수의 목적에 맞지 않습니다.

시그모이드 함수의 특징은 함수의 출력값이 0과 1사이의 값이라는점입니다.
즉, 실제값이 1일 때 예측값이 0에 가까워지면 오차가 커져야하며, 실제값이 0일 때, 예측값이 1에 가까워질수록 오차가 커져야 합니다.
바로... 이를 충족하는 함수가 로그 함수입니다.
아래는 $y = 0.5$에 대칭하는 두 개의 로그 함수 그래프입니다.

![[스크린샷 2026-01-20 오후 1.36.26.png]]

실제값이 1일 때의 그래프를 주황색, 실제값이 0일 때의 그래프를 초록색 선으로 표현한 것입니다.

실제값이 1인 경우, 예측값인 $H(x)$의 값이 1이면 오차가 0이므로 당연히 cost는 0이 됩니다.
반면, 실제값이 0으로 수렴하면 cost는 무한대로 발산합니다. 실제값이 0인 경우는 그 반대로 이해하면 됩니다. 이 두 개의 로그 함수를 식으로 표현하면 다음과 같습니다.

$$
\text{if} \space y = 1 \rightarrow cost(H(x),y) = -\log(H(x))
$$
$$
\text{if} \space y = 0 \rightarrow cost(H(x),y) = -\log(1 - H(x))
$$

이를 통합하면 다음과 같습니다.

$$
cost(H(x), y) = -[y\log {H(x)} + (1 - y)\log{(1 - H(x))}]
$$

이제 오차의 평균을 구해야 겠죠??

$$
cost(H(x)) = -{1 \over n}\sum_{i=1}^{n}{[y^{(i)}\log {H(x^{(i)})} + (1 - y^{i})\log{(1 - H(x^{(i)}))}]}
$$

이제 위 비용 함수에 대해 경사 하강법을 수행하면서 최적의 가중치 $W$를 찾아갑니다.

$$
W \coloneqq W - \alpha {{\partial} \over {\partial W}}cost(W)
$$

## 04. 파이토치로 로지스틱 회귀 구현하기
필요한 도구들을 임포트합니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
```

x_train과 y_train을 텐서로 선언합니다.

```python
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)
```

이제 가중치 $W$와 편향 $b$를 선언해야합니다.

```python
W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

자 이제 가설식을 세워 보겠습니다.
파이토치에서는 자연상수 $e^{x}$를 구현하기 위해서 torch.exp(x)를 사용합니다.

```python
hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))
```

지금 가중치 $W$와 편향 $b$는 `torch.zeros`를 통해 전부 0으로 초기화된 상태입니다. 이 상태에서 예측값을 출력해봅시다.

```python
print(hypothesis)
```

```bash
tensor([[0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000]], grad_fn=<MulBackward0>)
```

사실 이 가설식은 좀 더 간단히 구현할 수 있습니다. 이미 파이토치에서는 시그모이드 함수를 이미 구현하여 제공하고 있기 때문이죠.

```python
hypothesis = torch.sigmoid(x_train.matmul(W) + b)
```

```python
print(hypothesis)
```

```bash
tensor([[0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000]], grad_fn=<SigmoidBackward>)
```

두 예측값이 동일한 것을 알 수 있죠??

이제 비용 함수값인 현재 예측값과 실제값 사이의 cost를 구해보겠습니다.

$$
cost(H(x)) = -{1 \over n}\sum_{i=1}^{n}{[y^{(i)}\log {H(x^{(i)})} + (1 - y^{i})\log{(1 - H(x^{(i)}))}]}
$$

```python
print(hypothesis)
print(y_train)
```

```bash
tensor([[0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000],
        [0.5000]], grad_fn=<SigmoidBackward>)

tensor([[0.],
        [0.],
        [0.],
        [1.],
        [1.],
        [1.]])
```

직접 오차를 구해보도록 하겠습니다.

```python
losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))

print(losses)
```

```bash
tensor([[0.6931],
        [0.6931],
        [0.6931],
        [0.6931],
        [0.6931],
        [0.6931]], grad_fn=<NegBackward0>)
```

그리고 이제 이것들의 평균을 구하면 됩니다.

```python
cost = losses.mean()

print(cost)
```

```bash
tensor(0.6931, grad_fn=<MeanBackward1>)
```

결과적으로 얻은 cost는 0.6931 입니다.

하지만 이 마저도 파이토치에서 이미 구현되어 있습니다.
`torch.nn.functional as F`로 임포트 한 후에 `F.binary_cross_entropy(예측값, 실제값)`으로 사용하면 됩니다.

```python
cost = F.F.binary_cross_entropy(hypothesis, y_train)

print(cost)
```

```bash
tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)
```

같은 값이 출력되는 것을 볼 수 있죠?? 그럼 이제 모델의 훈련 과정까지 추가한 전체 코드는 아래와 같습니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

W = torch.zeros((2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)

optimizer = optim.SGD([W, b], lr=1)

epochs = 1000

for epoch in range(epochs + 1):
    hypothesis = torch.sigmoid(x_train.matmul(W) + b)

    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()

    optimizer.zero_grad()

    cost.backward()

    optimizer.step()

    # 100 번 마 다 로 그 출 력
    if epoch % 100 == 0:
        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, epochs, cost.item()))
```

```bash
Epoch    0/1000 Cost: 0.693147
Epoch  100/1000 Cost: 0.134722
Epoch  200/1000 Cost: 0.080643
Epoch  300/1000 Cost: 0.057900
Epoch  400/1000 Cost: 0.045300
Epoch  500/1000 Cost: 0.037261
Epoch  600/1000 Cost: 0.031673
Epoch  700/1000 Cost: 0.027556
Epoch  800/1000 Cost: 0.024394
Epoch  900/1000 Cost: 0.021888
Epoch 1000/1000 Cost: 0.019852
```

학습이 끝났습니다. 제대로 예측했는지 보기 위해 예측값을 출력해보죠.

```python
hypothesis = torch.sigmoid(x_train.matmul(W) + b)
print(hypothesis)
```

```bash
tensor([[2.7648e-04],
        [3.1608e-02],
        [3.8977e-02],
        [9.5622e-01],
        [9.9823e-01],
        [9.9969e-01]], grad_fn=<SigmoidBackward0>)
```

그럼 이제 예측값으로 0.5를 넘으면 Ture, 넘지 않으면 False로 출력해 보겠습니다.

```python
prediction = hypothesis >= torch.FloatTensor([0.5])
print(prediction)
```

```bash
tensor([[False],
        [False],
        [False],
        [ True],
        [ True],
        [ True]])
```

이젠 훈련된 이후의 $W$와 $b$의 값을 출력해 보겠습니다.

```python
print(W)
print(b)
```

```bash
tensor([[3.2530],
        [1.5179]], requires_grad=True)
tensor([-14.4819], requires_grad=True)
```