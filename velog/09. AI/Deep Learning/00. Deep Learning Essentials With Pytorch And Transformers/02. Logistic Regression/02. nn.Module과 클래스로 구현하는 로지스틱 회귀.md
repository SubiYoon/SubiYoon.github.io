---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
  - 로지스틱회귀
---
# nn.Module과 클래스로 구현하는 로지스틱 회귀

## 01.파이토치의 nn.Linear와 nn.Sigmoid로 로지스틱 회귀 구현하기
항상 가장 우선 구현을 위해 필요한 파이토치의 도구들을 임포트 했었죠?

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)
```

이제 데이터를 텐서로 선언합니다

```python
x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)
```

`nn.Sequential()`은 `nn.Module`층을 차례로 쌓을 수 있도록 합니다. 뒤에서 이를 이용해서 인공 신경망을 구현하게 되므로 기억하고 있으면 더욱 좋습니다.
쉽게 말하면 `nn.Sequential()`은 $Wx + b$와 같은 수식과 시그모이드 함수 등과 같은 여러함수들을 연결해주는 역할을 합니다.

```python
model = nn.Sequential(
    nn.Linear(2, 1), # input_dim = 2, output_dim = 1
    nn.Sigmoid() # 출력은 시그모이드 함수를 거친다.
)
```

현재 $W$와 $b$는 랜덤초기화 되어 있는 상태입니다. 훈련 데이터를 넣어 예측값을 확인해봅시다.

```python
hypothesis = model(x_train)
print(hypothesis)
```

```bash
tensor([[0.4020],
        [0.4147],
        [0.6556],
        [0.5948],
        [0.6788],
        [0.8061]], grad_fn=<SigmoidBackward>)
```

(6 x 1) 크기의 예측값 텐서가 출력됩니다.
하지만, 이 값들은 의미가 없습니다. 현재 $W$와 $b$는 임의의 값을 가지고 있기 때문이죠. 이제 경사 하강법을 사용하여 훈련을해보겠습니다.

```python
optimizer = optim.SGD(model.parameters(), lr=1)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):
    hypothesis = model(x_train)
    cost = F.binary_cross_entropy(hypothesis, y_train)
    
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    # 100 번 마 다 로 그 출 력
    if epoch % 100 == 0:
        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예 측 값 이 0.5 를 넘 으 면 True 로 간 주
        correct_prediction = prediction.float() == y_train # 실 제 값 과 일 치 하 는 경 우 만 True 로 간 주
        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정 확 도 를 계 산
        
        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에 포 크 마 다 정 확 도 를 출 력
        epoch, nb_epochs, cost.item(), accuracy * 100,))
```

```bash
Epoch    0/1000 Cost: 0.539713 Accuracy 83.33%
Epoch  100/1000 Cost: 0.134272 Accuracy 100.00%
Epoch  200/1000 Cost: 0.080486 Accuracy 100.00%
Epoch  300/1000 Cost: 0.057820 Accuracy 100.00%
Epoch  400/1000 Cost: 0.045251 Accuracy 100.00%
Epoch  500/1000 Cost: 0.037228 Accuracy 100.00%
Epoch  600/1000 Cost: 0.031649 Accuracy 100.00%
Epoch  700/1000 Cost: 0.027538 Accuracy 100.00%
Epoch  800/1000 Cost: 0.024381 Accuracy 100.00%
Epoch  900/1000 Cost: 0.021877 Accuracy 100.00%
Epoch 1000/1000 Cost: 0.019843 Accuracy 100.00%
```

한번 기존 훈련 데이터를 입력해 예측값을 확인해 보겠습니다.

```python
print(model(x_train))
```

```bash
tensor([[2.7616e-04], # → 0.0003
        [3.1595e-02], # → 0.0316
        [3.8959e-02], # → 0.0390
        [9.5624e-01], # → 0.9562
        [9.9823e-01], # → 0.9982
        [9.9969e-01]], # → 0.9997
        grad_fn=<SigmoidBackward0>)
```

예측값이 잘 나오는 것을 확인 할 수 있습니다. 그럼 훈련 후의 $W$와 $b$의 값을 확인해 볼까요??

```python
print(list(model.parameters()))
```

```bash
[Parameter containing:
tensor([[3.2534, 1.5181]], requires_grad=True), Parameter containing:
tensor([-14.4839], requires_grad=True)]
```

로지스틱 회귀를 `nn.Module`을 이용하지 않고 구현한 실습에서 얻었던 $W$와 $b$와 거의 일치합니다.

## 02. 인공 신경망으로 표현되는 로지스틱 회귀
로지스틱회귀는 인공 신경망으로 간주 할 수 있는데, 근거는 다음 이미지에서 보여집니다.

![[스크린샷 2026-01-21 오후 6.02.26.png]]

각 입력에 대해 검은색 화살표는 가중치, 회색 화살표는 편향이 곱해집니다.
무슨 말이냐면, 입력값 $x$는 가중치 $w$와 편향 $b$는 상수 1과 곱해지는 것으로 표현되는 것이죠.
그리고 출력 전 시그모이드 함수를 지나게 되는겁니다.

$$
H(x) = sigmoid(x_{1}w_{1} + x_{2}w_{2} + b)
$$

## 03. 모델을 클래스로 구현하기
파이토치의 대부분의 구현체들은 대부분 모델을 생성할 때 클래스(Class)를 사용합니다.
그럼 로지스틱 회귀를 클래스로 구현해 보도록 하겠습니다.

```python
import torch
import torch.nn as nn

class BinaryClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        return self.sigmoid(self.linear(x))
```

$H(x)$ 식에 입력 $x$로부터 예측된 $y$를 얻는 것을 `forward`라고 합니다.

## 04. 로지스틱 회귀 클래스로 구현하기
큰 차이는 없습니다. 달라진 점은 모델을 클래스로 구현했다는 것입니다.

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

torch.manual_seed(1)

x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
y_data = [[0], [0], [0], [1], [1], [1]]
x_train = torch.FloatTensor(x_data)
y_train = torch.FloatTensor(y_data)

class BinaryClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(2, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        return self.sigmoid(self.linear(x))

model = BinaryClassifier()

optimizer = optim.SGD(model.parameters(), lr=1)

nb_epochs = 1000
for epoch in range(nb_epochs + 1):
    
    # H(x) 계산 - 예측값 계산
    hypothesis = model(x_train)
    
    # cost 계산
    cost = F.binary_cross_entropy(hypothesis, y_train)
    
    # cost로 H(x) 개선
    optimizer.zero_grad()
    cost.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        prediction = hypothesis >= torch.FloatTensor([0.5]) # 예 측 값 이 0.5 를 넘 으 면 True 로 간 주
        correct_prediction = prediction.float() == y_train # 실 제 값 과 일 치 하 는 경 우 만 True 로 간 주
        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정 확 도 를 계 산
        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format( # 각 에 포 크 마 다 정 확 도 를 출 력
            epoch, nb_epochs, cost.item(), accuracy * 100,
        ))

```