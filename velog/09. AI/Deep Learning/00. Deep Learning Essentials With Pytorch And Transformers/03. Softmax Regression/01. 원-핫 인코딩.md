---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
  - 원-핫인코딩
---
# 원-핫 인코딩
## 01. 원-핫 인코딩(One-hot encoding)이란?
`원-핫 인코딩`은 선택해야 하는 선택지의 개수만큼 차원을 가지면서, 각 선택지의 인덱스에 해당하는 원소에는 1, 나머지 원소는 0의 값을 가지도록 하는 표현 방법입니다.

예를 들어 보겠습니다.

```bash
고양이 = [1, 0, 0]
고양이 = [0, 1, 0]
냉장고 = [0, 0, 1]
```

총 선택지는 3 개였으므로 위 벡터들은 전부 3 차원의 벡터가 되었습니다. 
3 차원의 벡터란 원소가 3개라는 의미입니다.

이와 같이 `원-핫 인코딩`으로 표현된 벡터를 `원-핫 벡터(one-hot vector)`라고 합니다.

## 02. 원-핫 벡터의 무작위성
꼭 실제값을 `원-핫 벡터`로 표현해야만 다중 클래스 분류 문제를 풀 수 있는 것은 아닙지만, 대부분의 다중 클래스 분류 문제가 각 클래스 간의 관계가 균등하다는 점에서 `원-핫 벡터`는 이러한 점을 표현할 수 있는 적절한 표현 방법입니다.

`Banana`, `Tomato`, `Apple`이라는 3개의 클래스가 존재하는 문제가 있따고 가정해봅시다.
레이블은 정수 인코딩을 사용하여 각각 1, 2, 3을 부여하였습니다.

$$
Loss \space function = {{1} \over {n}}\sum_{i}^{n}{(y_{i} - \hat{y}_{i})^{2}}
$$

직관적인 오차 크기 비교를 위해 평균을 구하는 수식은 제외하고 제곱 오차로만 판단해 봅시다.
실제값이 `Tomato`일때 예측앖이 `Banana`였다면 제곱 오차는 다음과 같습니다.

$$
(2 - 1)^{2} = 1
$$

실제값이 `APPLE`일때 예측값이 `Banana`였다면 제곱 오차는 다음과 같습니다.

$$
(3 - 1)^{2} = 4
$$

즉, `Banana`와 `Tomato`사이의 오차보다 `Banana`와 `Apple`의 오차가 더 큽니다. 이는 기계에게 `Banana`가 `Apple`보다는 `Tomato`에 더 가깝다는 정보를 주는 것과 다름없습니다.

예를 하나 더 들어보겠습니다.

```json
{
    Banana: 1,
    Tomato: 2,
    Apple: 3,
    Strawberry: 4,
     ...
    Watermelon: 10
}
```

이 정수 인코딩은 `Banana`가 `Watermelon`보다는 `Tomato`에 더 가깝다는 의미를 담고 있습니다.

정수 인코딩과 달리 `원-핫 인코딩`은 분류 문제 모든 클래스 간의 관계를 균등하게 배분합니다.

$$
((1, 0, 0) − (0, 1, 0))^{2} = (1 − 0)^{2} + (0 − 1)^{2} + (0 − 0)^{2} = 2
$$
$$
((1, 0, 0) − (0, 0, 1)){2} = (1 − 0){2} + (0 − 0){2} + (0 − 1){2} = 2
$$

다르게 표현하면 모든 클래스에 대해서 `원-핫 인코딩`을 통해 얻은 `원-핫 벡터`들은 모든 쌍에 대해서 유클리드 거리를 구해도 전부 유클리드 거리가 동일합니다. `원-핫 벡터`는 이처럼 각 클래스의 표현 방법이 무작위성을 가진다는 점을 표현할 수 있습니다.