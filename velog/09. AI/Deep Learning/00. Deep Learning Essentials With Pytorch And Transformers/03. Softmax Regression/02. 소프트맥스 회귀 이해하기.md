---
tags:
  - AI
  - DeepLearning
  - MachineLearning
  - eBook
  - 소프트맥스회귀
---
# 소프트맥스 회귀 이해하기
`로지스틱 회귀`가 2개의 선택지 중에서 1개를 고르는 `이진 분류(Binary Classification)`라면 `소프트맥스 회귀`는 3개 이상의 선택지 중에서 1개를 고르는 `다중 클래스 분류(Multi-Class Classification)`입니다.

## 01. 다중 클래스 분류(Multi-class Classification)
아래의 문제는 꽂받침 길이, 쪽받침 너비, 꽃잎 길이, 꽃잎 넓이라는 4개의 특성(feature)로 부터 setosa, versicolor, virginica라는 3개의 붓꽃 품종 중 어떤 품종인지를 예측하는 문제로 전형적인 다중 클래스 분류 문제 입니다.

![[스크린샷 2026-01-22 오후 5.30.39.png]]

위 붓꽃 품종 분류하기 문제를 어떻게 풀지 고민하기 위해 앞서 배운 로지스틱 회귀의 이진 분류를 복습해볼까요??

입력은 X, 가중치는 W, 편향은 B, 출력은 $\hat{Y}$으로 각 변수는 벡터 또는 행렬로 가정합니다.

### 로지스틱 회귀
로지스틱 회귀에서 시그모이드 함수는 예측값을 0과 1사이의 값으로 만듭니다.
예를 들어 스팸 메일을 필터링하는 분류기를 로지스틱 회귀를 통해 구현했을 경우, 출력이 0.75라면 이 메일이 스팸일 확률은 75%라는 의미 입니다.

![[스크린샷 2026-01-23 오전 9.47.25.png]]

가설 : $H(X) = sigmoid(WX + B)$

### 소프트맥스 회귀
소프트맥스 회귀는 확률의 총 합이 1이 되는 이 아이디어를 다중 클래스 분류 문제에 적용합니다.
소프트맥스 회귀는 각 클래스. 즉, 각 선택지마다 소수 확률을 할당합니다.
이 때 총 확률의 합은 1이 되어야 합니다.

![[스크린샷 2026-01-23 오전 11.20.10.png]]

`로지스틱회귀`의 `sigmoid`함수를 지나 처리하는 것처럼 `소프트맥스 회귀`도 어떤 함수를 지나게 만들어 해당 벡터가 벡터의 모든 원소의 합이 1이 되도록 해야합니다. 위의 붓꽃 품종 분류하기를 예로 든다면 선택지의 개수가 3개일때, 3차원 벡터가 어떤함수 `?`를 지나 원소의 총합이 1이 되게 해야하는 것이죠.
이 때 `?`함수를 `소프트맥스(softmax)`함수 라고합니다.

가설 : $H(X) = softmax(WX + B)$

## 02. 소프트맥스 함수(Softmax function)
소프트맥스 함수는 불류해야하는 정답지(클래스)의 총 개수를 $k$라고 할 때, $k$차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정합니다. 우선 수식에 대해 설명하고, 그 후에는 그림으로 이해해 보겠습니다.

### 소프트맥스 함수의 이해
$k$차원에서의 벡터에서 $i$번째 원소를 $a_{i}$ $i$번째 클래스가 정답을 확류을 $p_{i}$로 나타낸다고 하였을 때 소트프트 맥스 함수는 $p_{i}$를 다음과 같이 정의합니다.

$$
p_{i} = {{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}} \space for \space i = 1, 2, 3, \dots k
$$

위에서 풀어야하는 문제의 경우 $k$가 3이죠??
그러면 3차원 벡터 $z = \begin{bmatrix} z_{1} \space z_{2} \space z_{3} \end{bmatrix}$의 입력을 받으면 소프트맥스 함수는 아래와 같이 출력을 리턴합니다.

$$
softmax(z) = \begin{bmatrix}
{{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}} 
{{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}} 
{{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}}
\end{bmatrix}
=
\begin{bmatrix}
p_{1} \space p_{2} \space p_{3}
\end{bmatrix}
= \hat{y}
= \text{예측값}
$$

$p_{1}, \space p_{2}, \space p_{3}$ 각각 1,2,3번이 정답일 확률을 나타내며 각각 0과 1사이에 값으로 총합은 1이 됩니다.
여기서 분류하고자하는 3개의 클래스는 `virginica`, `setosa`, `versicolor`이므로 이는 결국 주어진 입력이 각각 `virginica`일 확률, `setosa`일 확률, `versicolor`일 확률을 나타내는 값을 의미합니다. 이에따라 식을 문제에 맞게 다시 쓰면 아래와 같습니다.

$$
softmax(z) = \begin{bmatrix}
{{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}} 
{{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}} 
{{e^{z_{i}}} \over {\sum_{j=1}^{k}{e^{z_{j}}}}}
\end{bmatrix}
=
\begin{bmatrix}
p_{1} \space p_{2} \space p_{3}
\end{bmatrix}
= 
\begin{bmatrix}
p_{virginica} \space p_{setosa} \space p_{versico\lor}
\end{bmatrix}
$$

### 그림을 통한 이해
![[스크린샷 2026-01-26 오전 10.05.54.png]]

위에 그림에서 다음 두가지의 의문을 가질 수 있습니다.

#### 소프트맥스 함수의 입력
하나의 샘플 데이터는 4개의 독립변수 $x$를 가지는데 이는 모델이 4차원 벡터를 입력으로 받음을 의미합니다. 그런데 우리는 소프트맥스 함수의 입력으로 사용되는 벡터는 벡터의 차원이 분류하고나하는 클래스의 개수가 되어야 하므로 특정 가중치 연산을 통해 3차원 벡터로 변환되어야 합니다.
위에 그림에서는 3차원 벡터로 변환시키기위해 사용되는 3차원 벡터를 $Z$로 표현했습니다.

![[스크린샷 2026-01-26 오전 10.16.34.png]]

샘플 데이터 벡터를 소프트맥스 함수의 입력 벡터로 차원을 축소하는 방법은 간단합니다.
소프트맥스 함수의 입력 벡터 $Z$의 차원수만큼  결과값이 나오도록 가중치를 곱하는것입니다.
위의 그림에서 화살표는 총 $(4 \times 3 = 12)$로 12개이며 전부 다른 가중치를 가지고, 학습 과정에서 점차적으로 오차픞 최소화하는 가중치로 값이 변경됩니다.

#### 오차 계산 방법
소프트맥스 함수의 출력은 클래스의 개수만큼 차원을 가지는 벡터로 각 $0 \sim 1$의 값을 가집니다.
그러면 예측값과 비교를 할 수 있는 실제값의 표현 방법이 있어야하는데 `소프트맥스 회귀`에서는 이 실제값을 `원-핫 벡터`로 표현합니다.

![[스크린샷 2026-01-26 오후 1.50.56.png]]

각각 `virginica`, `setosa`, `versicolor` 가 정답일 확률을 의미했을 때, 실제 값을 `원-핫 벡터`로 표현한 것입니다.

![[스크린샷 2026-01-26 오후 1.53.51.png]]

이렇게 예측값과 실제값의 두 벡터의 오차를 계산하귀 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용합니다.

![[스크린샷 2026-01-26 오후 1.55.38.png]]

이제 앞서 배운 선형 회귀나 로지스틱 회귀와 마찬가지로 오차로부터 가중치를 업데이트 합니다.

![[스크린샷 2026-01-26 오후 1.56.07.png]]


## 03. 붓꽃 품종 분류하기 행렬 연산으로 이해하기
우선 위의 예제 데이터를 $5 \times 4$행렬 $X$로 정의합니다.
그리고 편의를 위해 원소 위치르 반영한 변수로 표현을 같이 하겠습니다.

$$
X = 
\begin{pmatrix}
5.1 & 3.5 & 1.4 & 0.2 \\
4.9 & 3.0 & 1.4 & 0.2 \\
5.8 & 2.6 & 4.0 & 1.2 \\
6.7 & 3.0 & 5.2 & 2.3 \\
5.6 & 2.8 & 4.9 & 2.0
\end{pmatrix}
=
\begin{pmatrix}
x_{11} & x_{12} & x_{13} & x_{14} \\
x_{21} & x_{22} & x_{23} & x_{24} \\
x_{31} & x_{32} & x_{33} & x_{34} \\
x_{41} & x_{42} & x_{43} & x_{44} \\
x_{51} & x_{52} & x_{53} & x_{54} \\
\end{pmatrix}
$$

이젠 선택지가 3개이므로 예측값으로 얻는 행렬 $\hat{Y}$의 열의 개수는 3개여야 합니다.

$$
\hat{Y} =
\begin{pmatrix}
y_{11} & y_{12} & y_{13} \\
y_{21} & y_{22} & y_{23} \\
y_{31} & y_{32} & y_{33} \\
y_{41} & y_{42} & y_{43} \\
y_{51} & y_{52} & y_{53} \\
\end{pmatrix}
$$

그러면 가중치의 경우에는 $4 \times 3$행렬이 되어야 겠죠.

$$
W =
\begin{pmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33} \\
w_{41} & w_{42} & w_{43} \\
\end{pmatrix}
$$

그럼 편향은 어떨까요? $\hat{Y}$와 크기가 동일해야 하기 때문에 $5 \times 3$의 형태를 띄워야 겠죠??

$$
B =
\begin{pmatrix}
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
\end{pmatrix}
$$

결과적으로 가설식은 다음과 같습니다.

$\hat{Y} = softmax(XW + B)$

$$
\begin{pmatrix}
y_{11} & y_{12} & y_{13} \\
y_{21} & y_{22} & y_{23} \\
y_{31} & y_{32} & y_{33} \\
y_{41} & y_{42} & y_{43} \\
y_{51} & y_{52} & y_{53} \\
\end{pmatrix}

= softmax
\begin{pmatrix}
\begin{pmatrix}
x_{11} & x_{12} & x_{13} & x_{14} \\
x_{21} & x_{22} & x_{23} & x_{24} \\
x_{31} & x_{32} & x_{33} & x_{34} \\
x_{41} & x_{42} & x_{43} & x_{44} \\
x_{51} & x_{52} & x_{53} & x_{54} \\
\end{pmatrix}
\begin{pmatrix}
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23} \\
w_{31} & w_{32} & w_{33} \\
w_{41} & w_{42} & w_{43} \\
\end{pmatrix}
+ 
\begin{pmatrix}
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
b_{1} & b_{2} & b_{3} \\
\end{pmatrix}
\end{pmatrix}
$$

## 04. 비용함수(Cost function)
소프트맥스 회귀에서는 비용 함수로 크로스 엔트리 함수를 사용합니다.

### 크로스 엔트리 함수
$y$는 실제값, $k$는 클래스의 개수로 정의합니다.
$y_{j}$는 실제값 원-핫 벡터의 $j$번째 인덱스를 의미하며, $p_{j}$는 샘플데이터가 $j$번째 클래스일 활률을 나타냅니다. 표기에 따라서 $\hat{y_{j}}$로 표현하기도 합니다.

$$
cost(W) = -\sum_{j=1}^{k}{y_{j}\log{(p_{j})}}
$$

실제값 원-핫 벡터에서 1을 가진 원소의 인덱스의 $p$가 있다면 $p_{j} = 1$은 $\hat{y}$가 $y$를 정확하게 예측되는 경우가 됩니다. 그렇다면 $-1\log{1} = 0$이 되기 때문에 크로스 엔트로피 함수의 값은 0이 됩니다.
즉, $-\sum_{j=1}^{k}{y_{j}\log{(p_{j})}}$의 값을 최소화하는 방향으로 학습해야 합니다.
그럼 이제 최종 비용함수는 다음과 같습니다.

$$
cost(W) = -{{1} \over {n}}\sum_{i=1}^{n}\sum_{j=1}^{k}{y_{j}^{(i)}\log{(p_{j}^{(i)})}}
$$

### 이진 분류에서의 크로스 엔트로피 함수
로지스틱 회귀에서 배운 크로스 엔트리 함수식과 달라보이지만, 본질적으로는 동일한 함수식입니다.
한번 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출해볼까요??

$$
cost(H) = -(y\log {H(X)} + (1 - y)\log{(1 - H(X))})
$$

한번 변수를 치환해 보겠습니다.
- $y \rightarrow y_{1}$
- $1 - y \rightarrow y_{2}$
- $H(X) \rightarrow p_{1}$
- $1 - H(X) \rightarrow p_{2}$

$$
-(y_{1}\log{p_{1}} + y_{2}\log{p_{2}})
$$

이 식은 아래와 같이 표현할 수 있습니다.

$$
-\begin{pmatrix}
\sum_{i=1}^{2}{y_{i}\log{p_{i}}}
\end{pmatrix}
$$

여기에서 소프트맥스 회귀는 $k$의 값이 고정된 값이 아니므로 2를 $k$로 변경합니다.

$$
-(\sum_{i=1}^{k}{y_{i}\log{p_{i}}})
$$

역으로 소프크맥스 회귀에서 로지스틱 회귀의 크로스 엔트리 함수식을 얻는 것은 $k$를 2로하고, $y_{1}$과 $y_{2}$를 각각 $y$와 $1-y$로 치환하고, $p_{1}$과 $p_{2}$를 각각 $H(X)$와 $1-H(X)$로 치환하면 됩니다.

정리하면 소프트맥스 함수의 최종 비용함수에서 $k$가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같습니다.

$$
cost(W)
=
-{{1} \over {n}}\sum_{i=1}^{n}\sum_{j=1}^{k}{y_{j}^{(i)}\log{(p_{j}^{(i)})}}
=
-{{1} \over {n}}\sum_{i=1}^{n}{[y^{i}\log(p^{(i)}) + (1-y^{(i)})\log(1-p^{(i)})]}
$$